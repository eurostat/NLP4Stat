{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case A - Query builder (in progress)\n",
    "***\n",
    "* 2-grams, 3-grams, 4-grams\n",
    "* Glossary articles only - SE articles to be added soon\n",
    "* Many more matches expected with the addition of SE articles.\n",
    "* One way of showing user interaction - second one to be added soon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import os \n",
    "#import re\n",
    "#import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import pyodbc\n",
    "\n",
    "import gensim\n",
    "#import pprint\n",
    "#pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data read offline due to changes in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "GL_df = pd.read_excel('D3_2_Glossary_6_8_23_9.xlsx',index_col=0)\n",
    "GL_df['url'] = GL_df['url'].apply(lambda x: ast.literal_eval(x))\n",
    "##GL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, remove stop-words and stem; keep also the original terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "p = PorterStemmer()\n",
    "\n",
    "def text_to_words(text,url):\n",
    "    words = str(gensim.utils.simple_preprocess(text, deacc=True))\n",
    "    words = remove_stopwords(words) \n",
    "    words = gensim.utils.tokenize(words)\n",
    "        \n",
    "    ## keep also original token!!! \n",
    "    words = [[p.stem(token),token,url] for token in words if len(p.stem(token)) >= 5] ##minimum length = 5 \n",
    "    yield(words)        \n",
    "\n",
    "texts=list()    \n",
    "\n",
    "for i in range(len(GL_df)):\n",
    "    texts.extend(text_to_words(GL_df.loc[i,'definition'],GL_df.loc[i,'url']))\n",
    "    texts.extend(text_to_words(GL_df.loc[i,'title'],GL_df.loc[i,'url'])) \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurences: keys in n-grams are (n-1) tuples of stemmed tokens \n",
    "\n",
    "* Three dictionaries, for 2-,3-,and 4-grams\n",
    "* For each key in a dictionary, the value is another (nested) dictionary with the **original terms**, their counts and a lists of the relevant URLs.\n",
    "* Below all three dictionaries are constructed from the sequences of 4-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%%script false --no-raise-error\n",
    "## Check also COLLOCATIONS: http://www.nltk.org/howto/collocations.html and http://www.nltk.org/api/nltk.html?highlight=ngram\n",
    "\n",
    "\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "#from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "model2=dict()\n",
    "model3=dict()\n",
    "model4=dict()\n",
    "\n",
    "def dict_insert(model,entered,proposed,new_urls_to_check):\n",
    "    key_1 = model.get(entered)\n",
    "    if key_1:\n",
    "        key_2 = key_1.get(proposed)\n",
    "        if key_2:\n",
    "            key_2[0] +=1\n",
    "            existing_urls = key_2[1]\n",
    "            add_urls = [u for u in new_urls_to_check if u not in existing_urls]\n",
    "            key_2.extend(add_urls)\n",
    "        else:    \n",
    "            key_1[proposed]= [1,new_urls_to_check]\n",
    "    else:\n",
    "        model[entered]={proposed:[1,new_urls_to_check]}\n",
    "    return model            \n",
    "\n",
    "\n",
    "# Co-occurences\n",
    "\n",
    "for sentence in texts:\n",
    "    pairs = [elem for elem in sentence] ## a list of 3-tuples (stemmed term, original term, list of URLs)\n",
    "    if len(pairs) == 0: continue\n",
    "    \n",
    "    for first, second, third, fourth in ngrams(pairs,4): ## quadruplets of 3-tuples (stemmed term, original term, list of URLs)\n",
    "        first_stem, first_orig, first_urls = first\n",
    "        second_stem, second_orig, second_urls = second\n",
    "        third_stem, third_orig, third_urls = third\n",
    "        fourth_stem, fourth_orig, fourth_urls = fourth\n",
    "       \n",
    "        model2 = dict_insert(model2, first_stem, second_orig,list(set(first_urls).intersection(second_urls)))\n",
    "        model2 = dict_insert(model2, second_stem, third_orig,list(set(second_urls).intersection(third_urls)))\n",
    "        model2 = dict_insert(model2, third_stem, fourth_orig,list(set(third_urls).intersection(fourth_urls)))\n",
    "        \n",
    "        model3 = dict_insert(model3,(first_stem,second_stem),third_orig,list(set(first_urls).intersection(*[second_urls,third_urls])))\n",
    "        model3 = dict_insert(model3,(second_stem,third_stem),fourth_orig,list(set(second_urls).intersection(*[third_urls,fourth_urls])))\n",
    "        \n",
    "        model4 = dict_insert(model4,(first_stem, second_stem, third_stem),fourth_orig,fourth_urls)\n",
    "   \n",
    "        \n",
    "## Transform counts to probabilities\n",
    "\n",
    "for w1 in model2.keys():\n",
    "    ssum = sum(model2[w1][w2][0] for w2 in model2[w1].keys())\n",
    "    for w2 in model2[w1].keys():\n",
    "        model2[w1][w2][0] /= ssum\n",
    "\n",
    "for w1_w2 in model3.keys():\n",
    "    ssum = sum(model3[w1_w2][w3][0] for w3 in model3[w1_w2].keys())\n",
    "    for w3 in model3[w1_w2].keys():\n",
    "        model3[w1_w2][w3][0] /= ssum\n",
    "\n",
    "for w1_w2_w3 in model4.keys():\n",
    "    ssum = sum(model4[w1_w2_w3][w4][0] for w4 in model4[w1_w2_w3].keys())\n",
    "    for w4 in model4[w1_w2_w3].keys():\n",
    "        model4[w1_w2_w3][w4][0] /= ssum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Builder 1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load ipywidgets\n",
    "\n",
    "import ipywidgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact,Layout, IntSlider, interactive, widgets, interact_manual,HBox,fixed,VBox, Box, HTML\n",
    "from ipywidgets import Button, FloatText, Textarea, Dropdown, Label\n",
    "from ipywidgets import interact_manual\n",
    "layout = widgets.Layout(width='600px', height='30px')\n",
    "from IPython.display import HTML, display,clear_output\n",
    "from IPython.utils import io\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ba98d6a0c348fcb3963bfc5abe0710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='climate change', description='Keywords:', placeholder='Type something'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0128096b0bfe475ba4db1d8a54cccb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def change_top_articles( Keywords):\n",
    "\n",
    "    from operator import itemgetter\n",
    "    p = PorterStemmer()\n",
    "    \n",
    "    last_match = ''\n",
    "    \n",
    "    #if not Keywords.endswith(' '):\n",
    "    #    return None\n",
    "    \n",
    "    def test_and_back_step(x):\n",
    "        mod_index = -1\n",
    "        models = [model2,model3,model4]\n",
    "        if len(x)==1: \n",
    "            x=x[0] ; model=models[0]\n",
    "        elif len(x) ==2 or len(x) == 3:\n",
    "            x=tuple(x) ; mod_index=len(x)-1; model=models[mod_index]\n",
    "        elif len(x)==4:\n",
    "            x=tuple(x[:3]) ; model=models[2]\n",
    "        else:\n",
    "            return None\n",
    "        while not models[mod_index].get(x) and mod_index >=0:\n",
    "            x=x[:-1]\n",
    "            if len(x)==1 : x=x[0]\n",
    "            mod_index -=1 ; model=models[mod_index] \n",
    "        return (model,x)    \n",
    "            \n",
    "\n",
    "    x = Keywords.split() \n",
    "    if len(x) ==0: \n",
    "        print()\n",
    "        return\n",
    "    x = [p.stem(el) for el in x]\n",
    "    \n",
    "    \n",
    "    model,x = test_and_back_step(x)\n",
    "    if not model.get(x):\n",
    "        return None\n",
    "    \n",
    "\n",
    "    print()\n",
    "    print('Based on last match: ',x,'\\n')\n",
    "    print('Suggestions, probabilities (in descending order) and relevant URLs: ')\n",
    "    proposals = sorted([(k,v) for (k,v) in model[x].items()],key=itemgetter(1),reverse=True)\n",
    "    last_match = x\n",
    "    for key, value in proposals:\n",
    "        print()\n",
    "        print(key,': ',value[0])\n",
    "        for url in value[1]:\n",
    "            print(url)\n",
    "   \n",
    "    \n",
    "def query_build1(value):\n",
    "  style = {'description_width': 'initial'}\n",
    "    \n",
    "  Keywords = widgets.Text(\n",
    "      value=value,\n",
    "      placeholder='Type something',\n",
    "      description='Keywords:',\n",
    "      disabled=False\n",
    "  )\n",
    "\n",
    "  ui = widgets.HBox([Keywords])\n",
    "  out = widgets.interactive_output(change_top_articles, {'Keywords': Keywords})\n",
    "  display(ui, out)\n",
    "    \n",
    "query_build1(value='climate change')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

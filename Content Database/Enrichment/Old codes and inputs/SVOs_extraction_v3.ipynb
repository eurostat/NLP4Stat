{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Subject -   Verb - Object tuples related to Categories and Named Entities of Selected Classes\n",
    "\n",
    "### Step 1. Loading Spacy models\n",
    "***\n",
    "\n",
    "We install Spacy's language library for the first run. Then we can comment-out the download command. Note that we are loading Spacy's \"medium\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-md==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl#egg=en_core_web_md==3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-md==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: pathy in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: six in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: boto3 in d:\\programdata\\anaconda3\\lib\\site-packages (from smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.17.7)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.20.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.21.0,>=1.20.7->boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.8.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "Finished loading.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "## Run to install the language library, then comment-out\n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print('Finished loading.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Pre-processing\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    if pd.isnull(x): return x  \n",
    "    x = x.strip()\n",
    "\n",
    "    ## parentheses with only +/- digits, dots, spaces, commas, percentage sign, minus sign: replace with space\n",
    "    x = re.sub(r'\\([\\d\\+\\- \\.\\,%\\-]+\\)', ' ',x)\n",
    "    \n",
    "    ## delete ,000 commas in numbers    \n",
    "    x = re.sub(r'\\b(\\d+),(\\d+)\\b','\\\\1\\\\2',x)\n",
    "    \n",
    "    ## delete  000 spaces in numbers\n",
    "    x = re.sub(r'\\b(\\d+) (\\d+)\\b','\\\\1\\\\2',x)\n",
    "    \n",
    "    ## remove more than one spaces\n",
    "    x = re.sub(r' +', ' ',x)\n",
    "    \n",
    "    ## remove start and end spaces\n",
    "    x = re.sub(r'^ +| +$', '',x,flags=re.MULTILINE) \n",
    "    \n",
    "    ## space-comma -> comma\n",
    "    x = re.sub(r' \\,',',',x)\n",
    "    \n",
    "    ## space-dot -> dot\n",
    "    x = re.sub(r' \\.','.',x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Read the file _articles_5_23_20_27.xlsx_ with the fresh scraped content from the SE articles, i.e. the titles, URLs, abstracts, context sections, paragraph titles, full contents and related categories into a dataframe _SE_df_. This file was created with the existing spider and is easy to reproduce. In later versions, it will be created from the tables in the database. \n",
    "* Discard records with missing or duplicate titles and/or abstracts and/or raw contents (but *not* context sections which are frequently the same and some are missing) and do some data cleansing using function _clean()_. \n",
    "* Discard records which have empty strings in any of these columns (titles, abstracts, raw contents) after this data cleansing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>context</th>\n",
       "      <th>par titles</th>\n",
       "      <th>raw content</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult learning statistics</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>This article provides an overview of adult lea...</td>\n",
       "      <td>Lifelong learning can take place in a variety ...</td>\n",
       "      <td>Participation rate of adults in learning in th...</td>\n",
       "      <td>Participation rate of adults in learning in th...</td>\n",
       "      <td>['Education and training', 'Lifelong learning'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age of young people leaving their parental hou...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Leaving the parental home is considered as a m...</td>\n",
       "      <td>In addition to the Labour Force Survey (LFS), ...</td>\n",
       "      <td>Geographical differences. Gender differences. ...</td>\n",
       "      <td>Geographical differences. Map 1 indicates that...</td>\n",
       "      <td>['Household composition and family situation',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Administrative and support service statistics ...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>This article presents an overview of statistic...</td>\n",
       "      <td>The freedom to provide services and the freedo...</td>\n",
       "      <td>Structural profile. Sectoral analysis. Country...</td>\n",
       "      <td>Structural profile. In 2017 there were 1.4 mil...</td>\n",
       "      <td>['Services', 'Statistical article', 'Structura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adult learning statistics - characteristics of...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>This article presents an overview of European ...</td>\n",
       "      <td>Adults with a low level of educational attainm...</td>\n",
       "      <td>Formal and non-formal adult education and trai...</td>\n",
       "      <td>Formal and non-formal adult education and trai...</td>\n",
       "      <td>['Education and training', 'Participation in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accommodation and food service statistics - NA...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>This article presents an overview of statistic...</td>\n",
       "      <td>Tourism plays an important role in Europe and ...</td>\n",
       "      <td>Structural profile. Sectoral analysis. Country...</td>\n",
       "      <td>Structural profile. The accommodation and food...</td>\n",
       "      <td>['Services', 'Statistical article', 'Structura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Ageing Europe - statistics on social life and ...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Ageing Europe — looking at the lives of older ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Physical activity of older people. Older peopl...</td>\n",
       "      <td>Physical activity of older people. People at w...</td>\n",
       "      <td>['Statistical article', 'Poverty and social ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Ageing Europe - statistics on working and movi...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Ageing Europe — looking at the lives of older ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Employment patterns among older people. Focus ...</td>\n",
       "      <td>Employment patterns among older people. In 201...</td>\n",
       "      <td>['Statistical article', 'Labour market', 'Acci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Ageing Europe - statistics on health and disab...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Ageing Europe — looking at the lives of older ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life expectancy and healthy life years among o...</td>\n",
       "      <td>Life expectancy and healthy life years among o...</td>\n",
       "      <td>['Statistical article', 'Health', 'Mortality a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>Agri-environmental indicator - commitments</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>This article provides a fact sheet of the Euro...</td>\n",
       "      <td>Agri-environmental instruments are needed to s...</td>\n",
       "      <td>Key messages. Assessment.</td>\n",
       "      <td>Key messages. At the end of the Rural Developm...</td>\n",
       "      <td>['Agriculture', 'Environment', 'Environment an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>Ageing Europe - statistics on pensions, income...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Ageing Europe — looking at the lives of older ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pensions. Incomes for older people. The risk o...</td>\n",
       "      <td>Pensions. The transition for individuals from ...</td>\n",
       "      <td>['Statistical article', 'Social protection', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                            Adult learning statistics   \n",
       "1    Age of young people leaving their parental hou...   \n",
       "2    Administrative and support service statistics ...   \n",
       "3    Adult learning statistics - characteristics of...   \n",
       "4    Accommodation and food service statistics - NA...   \n",
       "..                                                 ...   \n",
       "587  Ageing Europe - statistics on social life and ...   \n",
       "588  Ageing Europe - statistics on working and movi...   \n",
       "589  Ageing Europe - statistics on health and disab...   \n",
       "590         Agri-environmental indicator - commitments   \n",
       "591  Ageing Europe - statistics on pensions, income...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1    https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "2    https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "3    https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "4    https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "..                                                 ...   \n",
       "587  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "588  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "589  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "590  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "591  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    This article provides an overview of adult lea...   \n",
       "1    Leaving the parental home is considered as a m...   \n",
       "2    This article presents an overview of statistic...   \n",
       "3    This article presents an overview of European ...   \n",
       "4    This article presents an overview of statistic...   \n",
       "..                                                 ...   \n",
       "587  Ageing Europe — looking at the lives of older ...   \n",
       "588  Ageing Europe — looking at the lives of older ...   \n",
       "589  Ageing Europe — looking at the lives of older ...   \n",
       "590  This article provides a fact sheet of the Euro...   \n",
       "591  Ageing Europe — looking at the lives of older ...   \n",
       "\n",
       "                                               context  \\\n",
       "0    Lifelong learning can take place in a variety ...   \n",
       "1    In addition to the Labour Force Survey (LFS), ...   \n",
       "2    The freedom to provide services and the freedo...   \n",
       "3    Adults with a low level of educational attainm...   \n",
       "4    Tourism plays an important role in Europe and ...   \n",
       "..                                                 ...   \n",
       "587                                                NaN   \n",
       "588                                                NaN   \n",
       "589                                                NaN   \n",
       "590  Agri-environmental instruments are needed to s...   \n",
       "591                                                NaN   \n",
       "\n",
       "                                            par titles  \\\n",
       "0    Participation rate of adults in learning in th...   \n",
       "1    Geographical differences. Gender differences. ...   \n",
       "2    Structural profile. Sectoral analysis. Country...   \n",
       "3    Formal and non-formal adult education and trai...   \n",
       "4    Structural profile. Sectoral analysis. Country...   \n",
       "..                                                 ...   \n",
       "587  Physical activity of older people. Older peopl...   \n",
       "588  Employment patterns among older people. Focus ...   \n",
       "589  Life expectancy and healthy life years among o...   \n",
       "590                          Key messages. Assessment.   \n",
       "591  Pensions. Incomes for older people. The risk o...   \n",
       "\n",
       "                                           raw content  \\\n",
       "0    Participation rate of adults in learning in th...   \n",
       "1    Geographical differences. Map 1 indicates that...   \n",
       "2    Structural profile. In 2017 there were 1.4 mil...   \n",
       "3    Formal and non-formal adult education and trai...   \n",
       "4    Structural profile. The accommodation and food...   \n",
       "..                                                 ...   \n",
       "587  Physical activity of older people. People at w...   \n",
       "588  Employment patterns among older people. In 201...   \n",
       "589  Life expectancy and healthy life years among o...   \n",
       "590  Key messages. At the end of the Rural Developm...   \n",
       "591  Pensions. The transition for individuals from ...   \n",
       "\n",
       "                                            categories  \n",
       "0    ['Education and training', 'Lifelong learning'...  \n",
       "1    ['Household composition and family situation',...  \n",
       "2    ['Services', 'Statistical article', 'Structura...  \n",
       "3    ['Education and training', 'Participation in e...  \n",
       "4    ['Services', 'Statistical article', 'Structura...  \n",
       "..                                                 ...  \n",
       "587  ['Statistical article', 'Poverty and social ex...  \n",
       "588  ['Statistical article', 'Labour market', 'Acci...  \n",
       "589  ['Statistical article', 'Health', 'Mortality a...  \n",
       "590  ['Agriculture', 'Environment', 'Environment an...  \n",
       "591  ['Statistical article', 'Social protection', '...  \n",
       "\n",
       "[592 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE_df = pd.read_excel('articles_5_23_20_27.xlsx')\n",
    "SE_df = SE_df[['title','url','abstract','context','Titles','Raw content','categories']]\n",
    "SE_df.rename(columns={'Titles':'par titles','Raw content':'raw content'},inplace=True)\n",
    "SE_df = SE_df.replace('', np.nan) \n",
    "\n",
    "SE_df = SE_df.dropna(axis=0,subset=['title','abstract','raw content'],how='any')\n",
    "\n",
    "SE_df = SE_df.drop_duplicates(subset=[\"title\"])\n",
    "SE_df = SE_df.drop_duplicates(subset=[\"abstract\"])\n",
    "SE_df = SE_df.drop_duplicates(subset=[\"raw content\"])\n",
    "\n",
    "SE_df['raw content'] = SE_df['raw content'].apply(clean)\n",
    "SE_df['abstract'] = SE_df['abstract'].apply(clean)\n",
    "SE_df['context'] = SE_df['context'].apply(clean)\n",
    "SE_df['par titles'] = SE_df['par titles'].apply(clean)\n",
    "SE_df['title'] = SE_df['title'].apply(lambda x: x.strip()) ## do not change anything else - reference field!\n",
    "\n",
    "SE_df = SE_df.replace('', np.nan) ## check if empty strings produced and drop records if necessary\n",
    "SE_df = SE_df.dropna(axis=0,subset=['title','abstract','raw content'],how='any')\n",
    "\n",
    "SE_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "SE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, read file _concepts_5_23_21_55.xlsx_ with the fresh scraped content from the SE Glossary articles, i.e. the titles, URLs, definitions and related categories into a dataframe _GL_df_. This file was created with the existing spider and is easy to reproduce. In later versions, it will be created from the tables in the database.\n",
    "* Discard records with missing titles and/or URLs and/or definitions and do some data cleansing of the definitions using function _clean()_. \n",
    "* Drop records with duplicate URLs. \n",
    "* Discard records with definitions which point to redirections ('Redirect to ...) or are the remnants of deleted articles ('The revision #...').\n",
    "* Discard duplicates in titles and definitions (which point to the same articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>definition</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accrual recording</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Accrual recording is the recording of the valu...</td>\n",
       "      <td>['Glossary', 'Short-term business statistics g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accidents to persons caused by rolling stock i...</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Accidents to one or more persons that are eith...</td>\n",
       "      <td>['Glossary', 'Statistical indicator', 'Transpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Active enterprises - FRIBS</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>&lt;Brief user-oriented definition, one or a few ...</td>\n",
       "      <td>['Under construction']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Activation policies</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>The activation policies are policies designed ...</td>\n",
       "      <td>['Economy and finance glossary', 'Glossary', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Active enterprise</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>An active enterprise is an enterprise that had...</td>\n",
       "      <td>['Economy and finance glossary', 'Glossary', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>Aggregate demand</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Aggregate demand is the total amount of goods ...</td>\n",
       "      <td>['Economy and finance glossary', 'Glossary', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>Age of vehicle</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Age of vehicle is the length of time after the...</td>\n",
       "      <td>['Glossary', 'Statistical indicator', 'Transpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>Adult education</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Adult education is specifically targeted at in...</td>\n",
       "      <td>['Education and training glossary', 'Glossary'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>Activity rate</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Activity rate is the percentage of active pers...</td>\n",
       "      <td>['Economy and finance glossary', 'Glossary', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>Activity limitation</td>\n",
       "      <td>https://ec.europa.eu/eurostat/statistics-expla...</td>\n",
       "      <td>Activity limitation is a dimension of health/d...</td>\n",
       "      <td>['Glossary', 'Health glossary']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1278 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                     Accrual recording   \n",
       "1     Accidents to persons caused by rolling stock i...   \n",
       "2                            Active enterprises - FRIBS   \n",
       "3                                   Activation policies   \n",
       "4                                     Active enterprise   \n",
       "...                                                 ...   \n",
       "1273                                   Aggregate demand   \n",
       "1274                                     Age of vehicle   \n",
       "1275                                    Adult education   \n",
       "1276                                      Activity rate   \n",
       "1277                                Activity limitation   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1     https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "2     https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "3     https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "4     https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "...                                                 ...   \n",
       "1273  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1274  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1275  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1276  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "1277  https://ec.europa.eu/eurostat/statistics-expla...   \n",
       "\n",
       "                                             definition  \\\n",
       "0     Accrual recording is the recording of the valu...   \n",
       "1     Accidents to one or more persons that are eith...   \n",
       "2     <Brief user-oriented definition, one or a few ...   \n",
       "3     The activation policies are policies designed ...   \n",
       "4     An active enterprise is an enterprise that had...   \n",
       "...                                                 ...   \n",
       "1273  Aggregate demand is the total amount of goods ...   \n",
       "1274  Age of vehicle is the length of time after the...   \n",
       "1275  Adult education is specifically targeted at in...   \n",
       "1276  Activity rate is the percentage of active pers...   \n",
       "1277  Activity limitation is a dimension of health/d...   \n",
       "\n",
       "                                             categories  \n",
       "0     ['Glossary', 'Short-term business statistics g...  \n",
       "1     ['Glossary', 'Statistical indicator', 'Transpo...  \n",
       "2                                ['Under construction']  \n",
       "3     ['Economy and finance glossary', 'Glossary', '...  \n",
       "4     ['Economy and finance glossary', 'Glossary', '...  \n",
       "...                                                 ...  \n",
       "1273  ['Economy and finance glossary', 'Glossary', '...  \n",
       "1274  ['Glossary', 'Statistical indicator', 'Transpo...  \n",
       "1275  ['Education and training glossary', 'Glossary'...  \n",
       "1276  ['Economy and finance glossary', 'Glossary', '...  \n",
       "1277                    ['Glossary', 'Health glossary']  \n",
       "\n",
       "[1278 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GL_df = pd.read_excel('concepts_5_23_21_55.xlsx')\n",
    "\n",
    "GL_df = GL_df[['title','url','definition','categories']]\n",
    "GL_df = GL_df.replace('', np.nan) \n",
    "GL_df = GL_df.dropna(axis=0,subset=['title','url','definition'],how='any')\n",
    "\n",
    "GL_df['title'] = GL_df['title'].apply(lambda x: x.strip())\n",
    "GL_df['url'] = GL_df['url'].apply(lambda x: x.strip())\n",
    "GL_df['definition'] = GL_df['definition'].apply(clean)\n",
    "\n",
    "GL_df = GL_df.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "idx = GL_df[GL_df['definition'].str.startswith('The revision #')].index\n",
    "GL_df.drop(idx , inplace=True)\n",
    "idx = GL_df[GL_df['definition'].str.startswith('Redirect to')].index\n",
    "GL_df.drop(idx , inplace=True)\n",
    "\n",
    "GL_df = GL_df.drop_duplicates(subset=[\"title\",\"definition\"])\n",
    "\n",
    "GL_df.reset_index(drop=True, inplace=True)\n",
    "GL_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a dataframe _Categories_SE_ with:\n",
    "    * the unique categories met in the SE articles in column _category_,\n",
    "    * their stemmed tokens, without stop-words, in column _category tokens_. \n",
    "    * Stemming is carried out with library _nltk_ because it is not available in Spacy. \n",
    "    * Drop the category _Statistical article_.\n",
    "* Do the same with the categories found in the SE Glossary articles (omitting the \"glossary\" in the end), drop the categories _Under construction_ and _Glossary_, create a dataframe _Categories_GL_, and \n",
    "* Merge the two dataframes into a _Categories_df_ dataframe dropping duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>category tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accidents at work</td>\n",
       "      <td>[accid, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acquisition of citizenship</td>\n",
       "      <td>[acquisit, citizenship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa</td>\n",
       "      <td>[africa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agricultural performance</td>\n",
       "      <td>[agricultur, perform]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>[agricultur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Statistical method</td>\n",
       "      <td>[statist, method]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Structural business statistics</td>\n",
       "      <td>[structur, busi, statist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Survey</td>\n",
       "      <td>[survey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Tourism</td>\n",
       "      <td>[tourism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Transport</td>\n",
       "      <td>[transport]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            category            category tokens\n",
       "0                  Accidents at work              [accid, work]\n",
       "1         Acquisition of citizenship    [acquisit, citizenship]\n",
       "2                             Africa                   [africa]\n",
       "3           Agricultural performance      [agricultur, perform]\n",
       "4                        Agriculture               [agricultur]\n",
       "..                               ...                        ...\n",
       "237               Statistical method          [statist, method]\n",
       "238  Structural business statistics   [structur, busi, statist]\n",
       "239                           Survey                   [survey]\n",
       "240                         Tourism                   [tourism]\n",
       "241                       Transport                 [transport]\n",
       "\n",
       "[242 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## create the Categories dataframe\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "import ast\n",
    "Categories_SE = pd.DataFrame(np.unique([el for i in range(len(SE_df)) \n",
    "                                        for el in ast.literal_eval(SE_df.loc[i,'categories'])]),\n",
    "                                        columns=['category'])\n",
    "Categories_SE['category tokens'] = Categories_SE['category'].apply(lambda x: \n",
    "                                                             [stemmer.stem(w.text.lower()) for w in nlp(str(x)) \n",
    "                                                             if not w.is_punct and not w.text.lower() in all_stopwords])                                                       \n",
    "\n",
    "Categories_SE.drop( Categories_SE[ Categories_SE['category'] == 'Statistical article' ].index, inplace=True)\n",
    "\n",
    "Categories_SE.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Categories_SE.to_excel('Categories_SE.xlsx')\n",
    "Categories_SE\n",
    "\n",
    "Categories_GL = pd.DataFrame(np.unique([el for i in range(len(GL_df)) \n",
    "                                        for el in ast.literal_eval(GL_df.loc[i,'categories'])]),\n",
    "                                        columns=['category'])\n",
    "Categories_GL['category'] = Categories_GL['category'].apply(lambda x: re.sub('glossary$','',x)) \n",
    "Categories_GL['category tokens'] = Categories_GL['category'].apply(lambda x: \n",
    "                                                             [stemmer.stem(w.text.lower()) for w in nlp(str(x)) \n",
    "                                                             if not w.is_punct and not w.text.lower() in all_stopwords])                                                       \n",
    "\n",
    "idx = Categories_GL[ (Categories_GL['category'] == 'Under construction') | (Categories_GL['category'] == 'Glossary') ].index\n",
    "Categories_GL.drop(idx , inplace=True)\n",
    "\n",
    "Categories_GL.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "Categories_GL.to_excel('Categories_GL.xlsx')\n",
    "Categories_GL\n",
    "\n",
    "Categories_df = pd.concat([Categories_SE,Categories_GL])\n",
    "Categories_df.drop_duplicates(subset=[\"category\"],inplace=True)\n",
    "Categories_df.reset_index(drop=True, inplace=True)\n",
    "del(Categories_SE, Categories_GL)\n",
    "Categories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. An improved version of a Subject-Verb-Object extraction function using Spacy\n",
    "***\n",
    "\n",
    "* By Peter de Vocht, see [GitHub code](https://github.com/peter3125/enhanced-subject-verb-object-extraction/blob/master/subject_verb_object_extract.py).\n",
    "* Function needs some **DESCRIPTION**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Peter de Vocht\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import en_core_web_sm\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# use spacy small model\n",
    "#nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "\n",
    "##ClearNLP Dependency Labels\n",
    "## https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
    "\n",
    "##https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf\n",
    "\n",
    "## https://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf\n",
    "\n",
    "# dependency markers for subjects\n",
    "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "## nominal subject, nominal subject passive, clausal subject, clausal subject passive, agent (e.g. killed by the \"agent\"), \n",
    "## expletive - an existential “there”\n",
    "\n",
    "# dependency markers for objects\n",
    "OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "## direct object, dative (indirect object), attr: “to be”, “to seem”, “to appear”, object predicate\n",
    "\n",
    "# POS tags that will break adjoining items\n",
    "BREAKER_POS = {\"CCONJ\", \"VERB\"}\n",
    "## coordinating conjunction, verb\n",
    "\n",
    "# words that are negations\n",
    "NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "\n",
    "\n",
    "# does dependency set contain any coordinating conjunctions?\n",
    "def contains_conj(depSet):\n",
    "    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n",
    "           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n",
    "\n",
    "\n",
    "# get subs joined by conjunctions\n",
    "def _get_subs_from_conjunctions(subs):\n",
    "    more_subs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights} \n",
    "        if contains_conj(rightDeps):\n",
    "            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_subs) > 0:\n",
    "                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n",
    "    return more_subs\n",
    "\n",
    "\n",
    "# get objects joined by conjunctions\n",
    "def _get_objs_from_conjunctions(objs):\n",
    "    more_objs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if contains_conj(rightDeps):\n",
    "            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_objs) > 0:\n",
    "                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n",
    "    return more_objs\n",
    "\n",
    "\n",
    "# find sub dependencies\n",
    "def _find_subs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"] ## !!! CHANGE: not stop-words ?\n",
    "        if len(subs) > 0:\n",
    "            verb_negated = _is_negated(head)\n",
    "            subs.extend(_get_subs_from_conjunctions(subs))\n",
    "            return subs, verb_negated\n",
    "        elif head.head != head:\n",
    "            return _find_subs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], _is_negated(tok)\n",
    "    return [], False\n",
    "\n",
    "\n",
    "# is the tok set's left or right negated?\n",
    "def _is_negated(tok):\n",
    "    parts = list(tok.lefts) + list(tok.rights)\n",
    "    for dep in parts:\n",
    "        if dep.lower_ in NEGATIONS:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# get all the verbs on tokens with negation marker\n",
    "def _find_svs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "\n",
    "# get grammatical objects for a given set of dependencies (including passive sentences)\n",
    "def _get_objs_from_prepositions(deps, is_pas):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or\n",
    "                         (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n",
    "                         (is_pas and tok.dep_ == 'pobj')])\n",
    "    return objs\n",
    "\n",
    "\n",
    "# get objects from the dependencies using the attribute dependency\n",
    "def _get_objs_from_attrs(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# xcomp; open complement - verb has no suject\n",
    "def _get_obj_from_xcomp(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# get all functional subjects adjacent to the verb passed in\n",
    "def _get_all_subs(v):\n",
    "    verb_negated = _is_negated(v)\n",
    "    ## !!! CHANGE: exclude stop-words ?\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(_get_subs_from_conjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verb_negated = _find_subs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verb_negated\n",
    "\n",
    "\n",
    "# find the main verb - or any aux verb if we can't find it\n",
    "## !!! CHANGE: exclude stop-words ?\n",
    "def _find_verbs(tokens):\n",
    "    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)] ### !!!\n",
    "    if len(verbs) == 0:\n",
    "        verbs = [tok for tok in tokens if _is_verb(tok)] ### !!!\n",
    "    \n",
    "    return verbs\n",
    "\n",
    "\n",
    "# is the token a verb?  (excluding auxiliary verbs)\n",
    "def _is_non_aux_verb(tok):\n",
    "    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n",
    "\n",
    "\n",
    "# is the token a verb?  (excluding auxiliary verbs)\n",
    "def _is_verb(tok):\n",
    "    return tok.pos_ == \"VERB\" or tok.pos_ == \"AUX\"\n",
    "\n",
    "\n",
    "# return the verb to the right of this verb in a CCONJ relationship if applicable\n",
    "# returns a tuple, first part True|False and second part the modified verb if True\n",
    "def _right_of_verb_is_conj_verb(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "\n",
    "    # VERB CCONJ VERB (e.g. he beat and hurt me)\n",
    "    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n",
    "        for tok in rights[1:]:\n",
    "            if _is_non_aux_verb(tok):\n",
    "                return True, tok\n",
    "\n",
    "    return False, v\n",
    "\n",
    "\n",
    "# get all objects for an active/passive sentence\n",
    "def _get_all_objs(v, is_pas):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n",
    "    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n",
    "    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n",
    "        objs.extend(potential_new_objs)\n",
    "        v = potential_new_verb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(_get_objs_from_conjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "\n",
    "# return true if the sentence is passive - at the moment a sentence is assumed passive if \n",
    "# it has an auxpass (auxiliary passive) verb\n",
    "def _is_passive(tokens):\n",
    "    for tok in tokens:\n",
    "        if tok.dep_ == \"auxpass\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# resolve a 'that' where/if appropriate\n",
    "def _get_that_resolution(toks):\n",
    "    for tok in toks:\n",
    "        if 'that' in [t.orth_ for t in tok.lefts]:\n",
    "            return tok.head\n",
    "    return None\n",
    "\n",
    "\n",
    "# simple stemmer using lemmas\n",
    "def _get_lemma(word: str):\n",
    "    tokens = nlp(word)\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0].lemma_\n",
    "    return word\n",
    "\n",
    "\n",
    "# print information for displaying all kinds of things of the parse tree\n",
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
    "\n",
    "\n",
    "# expand an obj / subj np using its chunk\n",
    "def expand(item, tokens, visited):\n",
    "    if item.lower_ == 'that':\n",
    "        temp_item = _get_that_resolution(tokens)\n",
    "        if temp_item is not None:\n",
    "            item = temp_item\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    if hasattr(item, 'lefts'):\n",
    "        for part in item.lefts:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    parts.append(item)\n",
    "\n",
    "    if hasattr(item, 'rights'):\n",
    "        for part in item.rights:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    if hasattr(parts[-1], 'rights'):\n",
    "        for item2 in parts[-1].rights:\n",
    "            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n",
    "                if item2.i not in visited:\n",
    "                    visited.add(item2.i)\n",
    "                    parts.extend(expand(item2, tokens, visited))\n",
    "            break\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "# convert a list of tokens to a string\n",
    "def to_str(tokens):\n",
    "    if isinstance(tokens, Iterable):\n",
    "        return ' '.join([item.text for item in tokens])\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    is_pas = _is_passive(tokens) ## is an \"auxpass\" verb contained in the tokens?\n",
    "    verbs = _find_verbs(tokens) ## get the main verbs (or aux verbs if none) \n",
    "    visited = set()  # recursion detection\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n",
    "            if isConjVerb:\n",
    "                v2, objs = _get_all_objs(conjV, is_pas)\n",
    "                for sub in subs:\n",
    "                    for obj in objs:\n",
    "                        objNegated = _is_negated(obj)\n",
    "                        if is_pas:  # reverse object / subject for passive\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                        else:\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n",
    "            else:\n",
    "                v, objs = _get_all_objs(v, is_pas)\n",
    "                for sub in subs:\n",
    "                    if len(objs) > 0:\n",
    "                        for obj in objs:\n",
    "                            objNegated = _is_negated(obj)\n",
    "                            if is_pas:  # reverse object / subject for passive\n",
    "                                svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                             \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            else:\n",
    "                                svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                             \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                    else:\n",
    "                        # no obj - just return the SV parts\n",
    "                        ## !!! CHANGE: return 'Object:None' as object\n",
    "                        svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                     \"!\" + v.lower_ if verbNegated else v.lower_,'Object:None'))\n",
    "                        #print('just return SV: ',(to_str(expand(sub, tokens, visited)),\n",
    "                        #             \"!\" + v.lower_ if verbNegated else v.lower_,))                              \n",
    "    \n",
    "    return svos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Apply the SVO function to the various texts and find tuples relevant to Named Entities and Categories \n",
    "***\n",
    "\n",
    "\n",
    "In each dataframe (SE_df and GL_df), create column **NER** which will hold dictionaries with the entities recognized as: \n",
    " * Companies, agencies, institutions, etc. (code ORG), \n",
    " * Countries, cities, states (code GPE), \n",
    " * Nationalities or religious or political groups (code NORP), \n",
    " * Non-GPE locations, mountain ranges, bodies of water (code LOCATION). \n",
    " * Buildings, airports, highways, bridges, etc. (code FACILITY),\n",
    " * Named hurricanes, battles, wars, sports events, etc. (code EVENT),\n",
    " * Named documents made into laws (code LAW),\n",
    " * Any named language (code LANGUAGE),\n",
    " * People, including fictional (code PERSON).\n",
    "\n",
    "In column **NER** in a record, the key is the entity and the values are:\n",
    "* a list with the tuples of the occurences of the entity (token span's *start* index position, token span's *stop* index position), \n",
    "* a list of the corresponding (coded) sources, and \n",
    "* the count of occurences in the content of the text processed.\n",
    "\n",
    "In each dataframe, we also create column **NER_SVOs** which will hold dictionaries with SVOs involving the above entities. In each dictionary in column NER_SVOs in a record, the key is the entity and the values are:\n",
    "* a list with the SVO tuples, \n",
    "* a list with the corresponding coded sources, \n",
    "* three lists with the titles, URLs and sentences  where the corresponding SVOs were found (for debugging purposes), \n",
    "* the count of occurences in the content of the text processed.\n",
    "\n",
    "Column **NER_SVOs** will also store keys of the form **\"Cat:category_name\"** corresponding to the **Categories**, with values **the lists of SVOs whose tokenized and stemmed terms have an [overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient) with some category's stemmed tokens** $\\ge$ 0.4. This value was found after experimentation. \n",
    "\n",
    "Finally, we also create a separate dictionary **Glob_NER_SVOs** gathering the above SVOs information from all texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_df['NER'] = [dict() for i in range(len(SE_df))]\n",
    "SE_df['NER_SVOs'] = [dict() for i in range(len(SE_df))]\n",
    "GL_df['NER'] = [dict() for i in range(len(GL_df))]\n",
    "GL_df['NER_SVOs'] = [dict() for i in range(len(GL_df))]\n",
    "Glob_NER_SVOs = dict() ## a separate dictionary holding all SVOs from all articles\n",
    "Cat_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Overlap(lst1, lst2):\n",
    "    return len(set(lst1).intersection(lst2))/min(len(set(lst1)),len(set(lst1)))\n",
    "\n",
    "def process_texts(dat,source,column):\n",
    "\n",
    "    nlp.max_length = 1500000\n",
    "    \n",
    "    for i in range(len(dat)):\n",
    "        if (i+1) % 100 == 0: print('article i = ',i+1,' of ',len(dat))\n",
    "        if all(dat.loc[i,[column]].isna()): continue    \n",
    "        doc = nlp(dat.loc[i,column]) ## pre-process text\n",
    "        url = dat.loc[i,'url']\n",
    "\n",
    "        sents = doc.sents ## segment into sentences\n",
    "        sents_list = [sent for sent in doc.sents]\n",
    "        num_sents = len(sents_list)\n",
    "        if num_sents ==0: \n",
    "            print(sents_list)\n",
    "            raise Exception(\"Error A!\") \n",
    "\n",
    "        for (j,sent) in enumerate(sents_list): ## Loop A over sentences #column 8\n",
    "            #----------------------------------------------------------\n",
    "            doc_sent = nlp(sent.text) ## pre-process sentence # column 12\n",
    "            \n",
    "            entities = doc_sent.ents ## general entities in sentence       \n",
    "            selected_ents=[]\n",
    "            if len(entities) > 0: ## otherwise proceed with SVOs vs. the categories only\n",
    "                for ent in entities: ## just a check to verify the span of each entity IN THE SENTENCE\n",
    "                    if ent.text != doc_sent.text[ent.start_char: ent.end_char]:\n",
    "                        raise Exception(\"Error B!\")             \n",
    "            \n",
    "                ## continue with selected named entities if any\n",
    "                selected_ents = [ent for ent in entities if ent.label_ in ['ORG','GPE','NORP','LOCATION','FACILITY','EVENT','LAW','LANGUAGE','PERSON']] ## selected  entities\n",
    "                ## cut +8.3, -17.4, 31353, etc.\n",
    "                selected_ents = [ent for ent in selected_ents if not re.search(r'^[\\d\\+\\-\\.\\,%\\-]+$',ent.text) ] \n",
    "\n",
    "\n",
    "            svos = findSVOs(doc_sent) \n",
    "            for sv in svos: ## loop B1 over SVOs in sentence\n",
    "            #--------------------------------------------------------------   \n",
    "                if sv[-1] == 'Object:None': \n",
    "                    continue\n",
    "                if '-' in sv or '%' in sv: \n",
    "                    continue\n",
    "                if any([x.startswith('Figure') or x.startswith('Table') for x in sv]):\n",
    "                    continue\n",
    "                if any([re.search(r'(\\d|\\.|\\+|\\-)+',x) for x in sv]):\n",
    "                    continue\n",
    "                if sum([1 for x in sv if x.lower() in all_stopwords])>=1:\n",
    "                    continue\n",
    "                    \n",
    "                ## open a parenthesis and then a number\n",
    "                sv = tuple(re.sub(r'(\\(|\\))$','',x) for x in sv)    \n",
    "                sv = tuple(re.sub(r'(\\(|\\))$','',x) for x in sv)  \n",
    "                #print(sv)\n",
    "                    \n",
    "                for s in sv: ## loop C1 over each SVO # column 16\n",
    "                #------------------------------------------------    \n",
    "                    #print('searching in: ',s)\n",
    "                    for e in selected_ents: ## loop D1 over each selected entity in an SVO # column 20\n",
    "                    #----------------------    \n",
    "                        #print('searching for ',e.text)\n",
    "                        if s.find(e.text) != -1:\n",
    "                            #print(sv,' : found ',e.text)\n",
    "                            key = e.text.upper()\n",
    "                            if key in dat.loc[i,'NER'].keys():\n",
    "                                dat.loc[i,'NER'][key][0].append((e.start,e.end)) \n",
    "                                dat.loc[i,'NER'][key][1].append(source) \n",
    "                                dat.loc[i,'NER'][key][2] += 1 \n",
    "                            else:    \n",
    "                                dat.loc[i,'NER'][key] = [[(e.start,e.end)],[source],1]\n",
    "                        \n",
    "                            if key in dat.loc[i,'NER_SVOs'].keys():\n",
    "                                if sv not in dat.loc[i,'NER_SVOs'][key][0]:\n",
    "                                    dat.loc[i,'NER_SVOs'][key][0].append(sv) \n",
    "                                    dat.loc[i,'NER_SVOs'][key][1].append(source) \n",
    "                                    dat.loc[i,'NER_SVOs'][key][2] += 1 \n",
    "                            else:    \n",
    "                                dat.loc[i,'NER_SVOs'][key] = [[sv],[source],1] \n",
    "                        \n",
    "                            ## global dictionary - avoid duplicates\n",
    "                            key = e.text.upper()\n",
    "                            if key in Glob_NER_SVOs.keys():\n",
    "                                if sv not in Glob_NER_SVOs[key][0]:\n",
    "                                    Glob_NER_SVOs[key][0].append(sv) \n",
    "                                    Glob_NER_SVOs[key][1].append(source)\n",
    "                                    Glob_NER_SVOs[key][2].append(dat.loc[i,'title'])\n",
    "                                    Glob_NER_SVOs[key][3].append(dat.loc[i,'url'])\n",
    "                                    Glob_NER_SVOs[key][4].append(sent.text)\n",
    "                                    Glob_NER_SVOs[key][5] += 1     \n",
    "                            else:    \n",
    "                                Glob_NER_SVOs[key] = [[sv],[source],[dat.loc[i,'title']],[dat.loc[i,'url']],[sent.text],1] \n",
    "                \n",
    "            \n",
    "                ## Continue loop C1 over each SVO # column 16, now with the Categories\n",
    "                sj = ' '.join(sv)\n",
    "                doc_sj = nlp(sj)\n",
    "                sj = [w.text.lower() for w in doc_sj if not w.is_punct]\n",
    "                sj = [w for w in sj if not w in all_stopwords]\n",
    "                sj = [stemmer.stem(w) for w in sj]\n",
    "                \n",
    "                # sj = [stemmer.stem(w.text.lower()) for w in doc_sj if not w.is_punct and not w.text.lower() in all_stopwords]\n",
    "                if len(sj) == 0: continue\n",
    "                #print('\\n',sv_copy)\n",
    "                ##print('sj = ',sj)\n",
    "                for m in range(len(Categories_df)): ## loop C2 over categories vs an SVO in a sentence\n",
    "                #-----------------------------------------------------------------------------------    \n",
    "                    ##print('cats:',categories_df.loc[m,'Category tokens'])\n",
    "                    try:\n",
    "                        overlap = Overlap(sj,Categories_df.loc[m,'category tokens'])\n",
    "                    except:\n",
    "                        print('sj = ',sj)\n",
    "                        print('m=',m)\n",
    "                        print('cats:',Categories_df.loc[m,'category tokens'])\n",
    "                        raise\n",
    "                    if overlap >= Cat_threshold:\n",
    "                        ##print('sj = ',sj)\n",
    "                        ##print(categories_df.loc[m,'Category'])\n",
    "                        key = 'Cat:'+Categories_df.loc[m,'category'].upper()\n",
    "                        if key in dat.loc[i,'NER_SVOs'].keys():\n",
    "                            if sv not in dat.loc[i,'NER_SVOs'][key][0]:\n",
    "                                dat.loc[i,'NER_SVOs'][key][0].append(sv) \n",
    "                                dat.loc[i,'NER_SVOs'][key][1].append(source) \n",
    "                                dat.loc[i,'NER_SVOs'][key][2] +=1\n",
    "                        else:\n",
    "                            dat.loc[i,'NER_SVOs'][key] = [[sv],[source],1]\n",
    "                            \n",
    "                        ## global dictionary \n",
    "                        if key in Glob_NER_SVOs.keys():\n",
    "                            if sv not in Glob_NER_SVOs[key][0]:\n",
    "                                Glob_NER_SVOs[key][0].append(sv) \n",
    "                                Glob_NER_SVOs[key][1].append(source) \n",
    "                                Glob_NER_SVOs[key][2].append(dat.loc[i,'title'])\n",
    "                                Glob_NER_SVOs[key][3].append(dat.loc[i,'url'])\n",
    "                                Glob_NER_SVOs[key][4].append(sent.text)                                \n",
    "                                Glob_NER_SVOs[key][5] += 1                             \n",
    "                        else:\n",
    "                            Glob_NER_SVOs[key] = [[sv],[source],[dat.loc[i,'title']],[dat.loc[i,'url']],[sent.text],1]                             \n",
    "                       \n",
    "    return dat                              \n",
    " \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PERSON People, including fictional\n",
    "#NORP Nationalities or religious or political groups\n",
    "#FACILITY Buildings, airports, highways, bridges, etc.\n",
    "#ORGANIZATION Companies, agencies, institutions, etc.\n",
    "#GPE Countries, cities, states\n",
    "#LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
    "#PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
    "#EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "#WORK OF ART Titles of books, songs, etc.\n",
    "#LAW Named documents made into laws \n",
    "#LANGUAGE Any named language\n",
    "#The following values are also annotated in a style similar to names:\n",
    "#DATE Absolute or relative dates or periods\n",
    "#TIME Times smaller than a day\n",
    "#PERCENT Percentage (including “%”)\n",
    "#MONEY Monetary values, including unit\n",
    "#QUANTITY Measurements, as of weight or distance\n",
    "#ORDINAL “first”, “second”\n",
    "#CARDINAL Numerals that do not fall under another typ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Apply this  procedure to the various texts\n",
    "***\n",
    "\n",
    "* Update column NER in both dataframes.\n",
    "* Update column NER_SVOs in both dataframes. \n",
    "* Update the separate global dictionary Glob_NER_SVOs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE articles titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  592\n",
      "article i =  200  of  592\n",
      "article i =  300  of  592\n",
      "article i =  400  of  592\n",
      "article i =  500  of  592\n"
     ]
    }
   ],
   "source": [
    "SE_df = process_texts(SE_df,'SE title','title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE articles paragraph titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  592\n",
      "article i =  200  of  592\n",
      "article i =  300  of  592\n",
      "article i =  400  of  592\n",
      "article i =  500  of  592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SE_df = process_texts(SE_df,'SE par. titles','par titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE articles abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  592\n",
      "article i =  200  of  592\n",
      "article i =  300  of  592\n",
      "article i =  400  of  592\n",
      "article i =  500  of  592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SE_df = process_texts(SE_df,'SE abstract','abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE articles context sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  592\n",
      "article i =  200  of  592\n",
      "article i =  300  of  592\n",
      "article i =  400  of  592\n",
      "article i =  500  of  592\n"
     ]
    }
   ],
   "source": [
    "SE_df = process_texts(SE_df,'SE context','context')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE articles full contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  592\n",
      "article i =  200  of  592\n",
      "article i =  300  of  592\n",
      "article i =  400  of  592\n",
      "article i =  500  of  592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SE_df = process_texts(SE_df,'SE content','raw content')\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE Glossary articles titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  1278\n",
      "article i =  200  of  1278\n",
      "article i =  300  of  1278\n",
      "article i =  400  of  1278\n",
      "article i =  500  of  1278\n",
      "article i =  600  of  1278\n",
      "article i =  700  of  1278\n",
      "article i =  800  of  1278\n",
      "article i =  900  of  1278\n",
      "article i =  1000  of  1278\n",
      "article i =  1100  of  1278\n",
      "article i =  1200  of  1278\n"
     ]
    }
   ],
   "source": [
    "GL_df = process_texts(GL_df,'GL title','title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE Glossary articles definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article i =  100  of  1278\n",
      "article i =  200  of  1278\n",
      "article i =  300  of  1278\n",
      "article i =  400  of  1278\n",
      "article i =  500  of  1278\n",
      "article i =  600  of  1278\n",
      "article i =  700  of  1278\n",
      "article i =  800  of  1278\n",
      "article i =  900  of  1278\n",
      "article i =  1000  of  1278\n",
      "article i =  1100  of  1278\n",
      "article i =  1200  of  1278\n"
     ]
    }
   ],
   "source": [
    "GL_df = process_texts(GL_df,'GL definition','definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Exporting the dataframes to Excel\n",
    "***\n",
    "\n",
    "This is also useful for the manual inspection and the design of rules for the fine-tuning of the NER engine and the SVO extraction. This output can then directly be imported in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now() \n",
    "outfile1 = 'SE_SVOs_'+str(current_time.month)+ '_' + str(current_time.day) + '_' + str(current_time.hour)+ '_' + str(current_time.minute)  +'.xlsx'\n",
    "outfile2 = 'GL_SVOs_'+str(current_time.month)+ '_' + str(current_time.day) + '_' + str(current_time.hour)+ '_' + str(current_time.minute)  +'.xlsx'\n",
    "#SE_df.to_excel(outfile1)\n",
    "#GL_df.to_excel(outfile2)\n",
    "\n",
    "SE_df.to_excel('SE_df.xlsx')\n",
    "GL_df.to_excel('GL_df.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Checking the dictionary with all SVOs collected\n",
    "***\n",
    "* And write all SVOs to both Excel and text files. The files include all information useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<A COUNCIL RECOMMENDATION> 1  entries\n",
      "<AAA> 1  entries\n",
      "<ACER> 1  entries\n",
      "<AEA> 8  entries\n",
      "<AEI> 3  entries\n",
      "<AES> 2  entries\n",
      "<AETIOLOGY> 1  entries\n",
      "<AF> 1  entries\n",
      "<AFGHAN> 4  entries\n",
      "<AFGHANS> 1  entries\n",
      "<AFRICAN> 7  entries\n",
      "<AGEING WORKING GROUP> 1  entries\n",
      "<AGENCY> 1  entries\n",
      "<AGRICULTURAL> 6  entries\n",
      "<AIC> 1  entries\n",
      "<AIREN> 1  entries\n",
      "<ALBANIA> 13  entries\n",
      "<ALBANIAN> 5  entries\n",
      "<ALBANIANS> 1  entries\n",
      "<ALENTEJO> 2  entries\n",
      "<ALGARVE> 1  entries\n",
      "<ALGECIRAS> 1  entries\n",
      "<ALGERIA> 15  entries\n",
      "<ALGERIAN> 2  entries\n",
      "<ALL EUROPEAN UNION> 1  entries\n",
      "<ALPS> 1  entries\n",
      "<AMIF> 1  entries\n",
      "<AMSTERDAM> 7  entries\n",
      "<AMSTERDAM SCHIPHOL> 1  entries\n",
      "<ANIMAL> 1  entries\n",
      "<ANTWERPEN> 7  entries\n",
      "<ARA> 1  entries\n",
      "<ARABIC> 1  entries\n",
      "<ARGENTINA> 1  entries\n",
      "<ARMENIA> 17  entries\n",
      "<AROPE> 2  entries\n",
      "<ASEAN> 20  entries\n",
      "<ASEM> 25  entries\n",
      "<ASIAN> 13  entries\n",
      "<ASSOCIATION AGREEMENTS> 1  entries\n",
      "<ATHENS> 1  entries\n",
      "<ATTIKI> 1  entries\n",
      "<AUSTRALIA> 3  entries\n",
      "<AUSTRALIAN> 1  entries\n",
      "<AUSTRIA> 62  entries\n",
      "<AUSTRIAN> 1  entries\n",
      "<AVIARIES> 1  entries\n",
      "<AWU> 1  entries\n",
      "<AZERBAIJAN> 11  entries\n",
      "<BACKFILLING> 1  entries\n",
      "<BALKAN> 1  entries\n",
      "<BALTIC> 18  entries\n",
      "<BANGKOK> 1  entries\n",
      "<BANGLADESH> 6  entries\n",
      "<BANK> 1  entries\n",
      "<BARCELONA> 5  entries\n",
      "<BARCELONA EUROPEAN COUNCIL> 1  entries\n",
      "<BEC> 1  entries\n",
      "<BEIJING> 1  entries\n",
      "<BELARUS> 22  entries\n",
      "<BELGIAN> 3  entries\n",
      "<BELGIUM> 82  entries\n",
      "<BENELUX> 3  entries\n",
      "<BERGEN> 1  entries\n",
      "<BERLIN> 3  entries\n",
      "<BERMUDA> 1  entries\n",
      "<BEVERIDGE> 3  entries\n",
      "<BIS> 2  entries\n",
      "<BMI> 6  entries\n",
      "<BOCHUM> 1  entries\n",
      "<BOD> 1  entries\n",
      "<BOP> 4  entries\n",
      "<BOSNIA> 11  entries\n",
      "<BRACKISHWATER CULTURE> 1  entries\n",
      "<BRATISLAVSKÝ> 1  entries\n",
      "<BRAUNSCHWEIG> 4  entries\n",
      "<BRAZIL> 3  entries\n",
      "<BRETAGNE> 1  entries\n",
      "<BRITISH> 3  entries\n",
      "<BRUGES> 1  entries\n",
      "<BRUNEI DARUSSALAM> 1  entries\n",
      "<BUDAPEST> 1  entries\n",
      "<BULGARIA> 154  entries\n",
      "<BULGARIAN> 1  entries\n",
      "<BULGARIANS> 1  entries\n",
      "<CALABRIA> 1  entries\n",
      "<CALAIS> 1  entries\n",
      "<CAMBODIA> 2  entries\n",
      "<CAMPANIA> 1  entries\n",
      "<CANADA> 3  entries\n",
      "<CANARIAS> 2  entries\n",
      "<CANCON> 1  entries\n",
      "<CAP> 8  entries\n",
      "<CATALUÑA> 3  entries\n",
      "<CC> 3  entries\n",
      "<CCI> 5  entries\n",
      "<CCP> 1  entries\n",
      "<CEDEFOP> 1  entries\n",
      "<CEPA> 2  entries\n",
      "<CFP> 5  entries\n",
      "<CFSP> 1  entries\n",
      "<CHEMNITZ> 1  entries\n",
      "<CHINA> 41  entries\n",
      "<CHINESE> 6  entries\n",
      "<CHP> 1  entries\n",
      "<CIRCULAR> 1  entries\n",
      "<CIS> 3  entries\n",
      "<CLRTAP> 1  entries\n",
      "<CMR> 2  entries\n",
      "<CMU> 2  entries\n",
      "<COFOG> 4  entries\n",
      "<COHESION> 1  entries\n",
      "<COICOP> 3  entries\n",
      "<COM> 1  entries\n",
      "<COMEXT> 1  entries\n",
      "<COMMISSION> 36  entries\n",
      "<COMMISSION COMMUNICATION> 1  entries\n",
      "<COMMISSION REGULATION> 1  entries\n",
      "<COMMISSIONERS> 2  entries\n",
      "<COMMUNICATION> 25  entries\n",
      "<COMMUNICATION INVESTING> 1  entries\n",
      "<COMMUNIST> 1  entries\n",
      "<COMMUNITY> 2  entries\n",
      "<CONSTANTA> 1  entries\n",
      "<CONVENTION> 2  entries\n",
      "<COPENHAGEN> 5  entries\n",
      "<COSTA RICA> 1  entries\n",
      "<COUNCIL> 16  entries\n",
      "<COUNCIL DECISION> 1  entries\n",
      "<COUNCIL REGULATION> 3  entries\n",
      "<COUNCIL REGULATIONS> 1  entries\n",
      "<CPPI> 2  entries\n",
      "<CROATIA> 97  entries\n",
      "<CROATIAN> 2  entries\n",
      "<CROPLANDS> 1  entries\n",
      "<CROPS> 2  entries\n",
      "<CSA> 1  entries\n",
      "<CT> 2  entries\n",
      "<CVTS> 1  entries\n",
      "<CYPRUS> 5  entries\n",
      "<CZECH> 7  entries\n",
      "<CZECHIA> 76  entries\n",
      "<Cat:ACCIDENTS AT WORK> 22  entries\n",
      "<Cat:ACQUISITION OF CITIZENSHIP> 1  entries\n",
      "<Cat:AGRICULTURAL PERFORMANCE> 2  entries\n",
      "<Cat:AGRICULTURE STATISTICS BY AREA AND REGION> 46  entries\n",
      "<Cat:AGRICULTURE, FORESTRY AND FISHERIES> 11  entries\n",
      "<Cat:AIR POLLUTION> 13  entries\n",
      "<Cat:ASYLUM AND DUBLIN PROCEDURE> 2  entries\n",
      "<Cat:AUTHORED ARTICLE> 5  entries\n",
      "<Cat:BALANCE OF PAYMENTS> 4  entries\n",
      "<Cat:BALANCE OF PAYMENTS > 4  entries\n",
      "<Cat:BALANCE OF PAYMENTS BY AREA> 12  entries\n",
      "<Cat:BUSINESS DEMOGRAPHY> 5  entries\n",
      "<Cat:CITIES> 1  entries\n",
      "<Cat:CLIMATE CHANGE> 17  entries\n",
      "<Cat:COMPARATIVE PRICE LEVELS (PPPS)> 18  entries\n",
      "<Cat:CONSTRUCTION SHORT-TERM STATISTICS> 9  entries\n",
      "<Cat:CONSUMER PRICE INDICES AND INFLATION> 26  entries\n",
      "<Cat:CONSUMER PRICE INDICES AND INFLATION BY AREA> 40  entries\n",
      "<Cat:CONSUMER PRICES> 4  entries\n",
      "<Cat:CONSUMER PRICES > 4  entries\n",
      "<Cat:CROP AND ANIMAL PRODUCTION> 9  entries\n",
      "<Cat:DEMOGRAPHY AND FAMILY - YOUNG PEOPLE> 80  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY> 3  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY > 3  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY - ENTERPRISES> 17  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY - HOUSEHOLDS> 13  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY - YOUNG PEOPLE> 82  entries\n",
      "<Cat:DIGITAL ECONOMY AND SOCIETY STATISTICS BY AREA AND REGION> 37  entries\n",
      "<Cat:DISTRIBUTIVE TRADE > 1  entries\n",
      "<Cat:DISTRIBUTIVE TRADES> 1  entries\n",
      "<Cat:E-BUSINESS> 2  entries\n",
      "<Cat:ECONOMIC DIMENSION OF CULTURE> 1  entries\n",
      "<Cat:ECONOMY AND FINANCE> 1  entries\n",
      "<Cat:ECONOMY AND FINANCE > 1  entries\n",
      "<Cat:EDUCATION - YOUNG PEOPLE> 89  entries\n",
      "<Cat:EDUCATION AND TRAINING> 8  entries\n",
      "<Cat:EDUCATION AND TRAINING > 8  entries\n",
      "<Cat:EDUCATION STATISTICS BY AREA AND REGION> 38  entries\n",
      "<Cat:EDUCATIONAL ATTAINMENT> 20  entries\n",
      "<Cat:ELECTRICITY> 2  entries\n",
      "<Cat:EMPLOYMENT> 4  entries\n",
      "<Cat:EMPLOYMENT BY SECTOR> 21  entries\n",
      "<Cat:ENERGY AND ENVIRONMENT> 1  entries\n",
      "<Cat:ENERGY CONSUMPTION> 11  entries\n",
      "<Cat:ENERGY PRICES> 2  entries\n",
      "<Cat:ENERGY PRODUCTION AND IMPORTS> 41  entries\n",
      "<Cat:ENERGY STATISTICS BY AREA> 22  entries\n",
      "<Cat:ENVIRONMENT - ECONOMIC ISSUES> 4  entries\n",
      "<Cat:EU ENLARGEMENT COUNTRIES> 52  entries\n",
      "<Cat:EU INSTITUTIONS > 12  entries\n",
      "<Cat:EUROPE 2020 INDICATORS> 2  entries\n",
      "<Cat:EUROPEAN NEIGHBOURHOOD POLICY COUNTRIES> 28  entries\n",
      "<Cat:EXCHANGE AND INTEREST RATES> 33  entries\n",
      "<Cat:EXCHANGE AND INTEREST RATES > 33  entries\n",
      "<Cat:EXPERIMENTAL STATISTICS> 4  entries\n",
      "<Cat:FERTILITY AND BIRTHS> 1  entries\n",
      "<Cat:FIRST AND SECOND-GENERATION IMMIGRANTS> 24  entries\n",
      "<Cat:FOREIGN CONTROLLED ENTERPRISES> 27  entries\n",
      "<Cat:FOREIGN DIRECT INVESTMENT (FDI)> 10  entries\n",
      "<Cat:GDP BY AREA AND REGION> 19  entries\n",
      "<Cat:GLOBALISATION IN BUSINESSES> 3  entries\n",
      "<Cat:GOVERNMENT EXPENDITURE BY FUNCTION> 10  entries\n",
      "<Cat:GOVERNMENT FINANCE BY AREA> 13  entries\n",
      "<Cat:GOVERNMENT FINANCE STATISTICS> 14  entries\n",
      "<Cat:GOVERNMENT FINANCE STATISTICS > 14  entries\n",
      "<Cat:HEALTH> 1  entries\n",
      "<Cat:HEALTH > 1  entries\n",
      "<Cat:HEALTH - YOUNG PEOPLE> 91  entries\n",
      "<Cat:HEALTH AND SAFETY> 2  entries\n",
      "<Cat:HEALTH CARE> 9  entries\n",
      "<Cat:HEALTH DETERMINANTS - ENVIRONMENT> 9  entries\n",
      "<Cat:HEALTH DETERMINANTS - LIFESTYLES> 10  entries\n",
      "<Cat:HEALTH EXPENDITURE> 7  entries\n",
      "<Cat:HEALTH STATISTICS BY AREA AND REGION> 35  entries\n",
      "<Cat:HEALTH STATUS> 10  entries\n",
      "<Cat:HOUSE SALES STATISTICS> 6  entries\n",
      "<Cat:HOUSEHOLD COMPOSITION AND FAMILY SITUATION> 17  entries\n",
      "<Cat:HOUSEHOLD CONSUMPTION AND ASSETS> 16  entries\n",
      "<Cat:HOUSEHOLD INCOME, EXPENDITURE AND DEBT> 47  entries\n",
      "<Cat:HOUSING> 3  entries\n",
      "<Cat:HOUSING PRICES> 8  entries\n",
      "<Cat:IMMIGRATION LAW ENFORCEMENT> 2  entries\n",
      "<Cat:INDUSTRIAL PRODUCTION> 8  entries\n",
      "<Cat:INDUSTRY AND CONSTRUCTION> 2  entries\n",
      "<Cat:INDUSTRY AND CONSTRUCTION > 2  entries\n",
      "<Cat:INDUSTRY AND SERVICES> 10  entries\n",
      "<Cat:INDUSTRY SHORT-TERM STATISTICS> 10  entries\n",
      "<Cat:INFRASTRUCTURE AND MEANS> 1  entries\n",
      "<Cat:INLAND WATERWAYS> 8  entries\n",
      "<Cat:INTERNATIONAL ORGANISATIONS > 5  entries\n",
      "<Cat:INTERNATIONAL TRADE> 20  entries\n",
      "<Cat:INTERNATIONAL TRADE > 20  entries\n",
      "<Cat:INTERNATIONAL TRADE IN SERVICES BY ENTERPRISES> 79  entries\n",
      "<Cat:INTERNATIONAL TRADE IN SERVICES BY PARTNER> 54  entries\n",
      "<Cat:INTERNATIONAL TRADE IN SERVICES BY TYPE OF SERVICE> 51  entries\n",
      "<Cat:INTERNATIONALLY TRADING ENTERPRISES> 42  entries\n",
      "<Cat:JOB VACANCIES> 5  entries\n",
      "<Cat:LABOUR COST> 12  entries\n",
      "<Cat:LABOUR MARKET> 37  entries\n",
      "<Cat:LABOUR MARKET > 37  entries\n",
      "<Cat:LABOUR MARKET - YOUNG PEOPLE> 117  entries\n",
      "<Cat:LABOUR MARKET STATISTICS BY AREA AND REGION> 72  entries\n",
      "<Cat:LABOUR MOBILITY> 1  entries\n",
      "<Cat:LANGUAGE LEARNING> 14  entries\n",
      "<Cat:LIFELONG LEARNING> 7  entries\n",
      "<Cat:LIVING CONDITIONS> 18  entries\n",
      "<Cat:LIVING CONDITIONS > 18  entries\n",
      "<Cat:LIVING CONDITIONS - YOUNG PEOPLE> 160  entries\n",
      "<Cat:MATERIAL FLOWS> 12  entries\n",
      "<Cat:MIGRANT INTEGRATION> 4  entries\n",
      "<Cat:MIGRATION AND MIGRANT POPULATION> 4  entries\n",
      "<Cat:MIGRATION BY AREA AND REGION> 16  entries\n",
      "<Cat:MONETARY AND OTHER FINANCIAL STATISTICS > 3  entries\n",
      "<Cat:MORTALITY AND LIFE EXPECTANCY> 12  entries\n",
      "<Cat:NATIONAL ACCOUNTS > 38  entries\n",
      "<Cat:NATIONAL ACCOUNTS (INCL. GDP)> 40  entries\n",
      "<Cat:NATURAL GAS> 20  entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Cat:NON-EU COUNTRIES> 40  entries\n",
      "<Cat:PAGES USING DUPLICATE ARGUMENTS IN TEMPLATE CALLS> 1  entries\n",
      "<Cat:PAGES WITH BROKEN FILE LINKS> 2  entries\n",
      "<Cat:PARTICIPATION IN CULTURE> 6  entries\n",
      "<Cat:PARTICIPATION IN EDUCATION AND TRAINING> 12  entries\n",
      "<Cat:POPULATION> 1  entries\n",
      "<Cat:POPULATION > 1  entries\n",
      "<Cat:POPULATION AGEING> 27  entries\n",
      "<Cat:POPULATION AND SOCIAL CONDITIONS> 4  entries\n",
      "<Cat:POPULATION BY AREA AND REGION> 34  entries\n",
      "<Cat:POPULATION SIZE AND PROJECTIONS> 13  entries\n",
      "<Cat:POSTAL STATISTICS > 3  entries\n",
      "<Cat:POVERTY AND SOCIAL EXCLUSION> 16  entries\n",
      "<Cat:PRICE LEVELS BY CONSUMPTION GROUPS> 21  entries\n",
      "<Cat:PRODUCTION STATISTICS> 10  entries\n",
      "<Cat:QUALITY OF LIFE> 21  entries\n",
      "<Cat:REGIONAL YEARBOOK> 4  entries\n",
      "<Cat:REGIONS - AGRICULTURE> 5  entries\n",
      "<Cat:REGIONS - ECONOMY AND FINANCE> 8  entries\n",
      "<Cat:REGIONS - EDUCATION AND TRAINING> 13  entries\n",
      "<Cat:REGIONS - HEALTH> 5  entries\n",
      "<Cat:REGIONS - LABOUR MARKET> 42  entries\n",
      "<Cat:REGIONS - POPULATION> 13  entries\n",
      "<Cat:REGIONS - TRANSPORT> 5  entries\n",
      "<Cat:REGIONS AND CITIES> 16  entries\n",
      "<Cat:REGIONS AND CITIES > 16  entries\n",
      "<Cat:RESEARCH AND DEVELOPMENT> 1  entries\n",
      "<Cat:RESIDENCE PERMITS> 44  entries\n",
      "<Cat:RESIDENTS' TRIPS AND DESTINATIONS> 8  entries\n",
      "<Cat:RETAIL TRADE SHORT-TERM STATISTICS> 20  entries\n",
      "<Cat:S & T BY AREA AND REGION> 16  entries\n",
      "<Cat:SBS BY SIZE CLASS> 4  entries\n",
      "<Cat:SCIENCE AND TECHNOLOGY EMPLOYMENT> 9  entries\n",
      "<Cat:SECTOR ACCOUNTS> 13  entries\n",
      "<Cat:SERVICES> 6  entries\n",
      "<Cat:SERVICES > 6  entries\n",
      "<Cat:SERVICES SHORT-TERM STATISTICS> 22  entries\n",
      "<Cat:SHORT-TERM BUSINESS STATISTICS> 22  entries\n",
      "<Cat:SHORT-TERM BUSINESS STATISTICS > 22  entries\n",
      "<Cat:SHORT-TERM BUSINESS STATISTICS BY AREA> 36  entries\n",
      "<Cat:SOCIAL PARTICIPATION> 6  entries\n",
      "<Cat:SOCIAL PROTECTION> 12  entries\n",
      "<Cat:SOCIAL PROTECTION > 12  entries\n",
      "<Cat:SOCIAL PROTECTION BENEFITS> 46  entries\n",
      "<Cat:SOIL, LAND COVER AND LAND USE> 63  entries\n",
      "<Cat:STATISTICAL CONCEPT> 5  entries\n",
      "<Cat:STATISTICAL INDICATOR> 20  entries\n",
      "<Cat:STATISTICAL METHOD> 5  entries\n",
      "<Cat:STRUCTURAL BUSINESS STATISTICS> 30  entries\n",
      "<Cat:STRUCTURAL BUSINESS STATISTICS > 30  entries\n",
      "<Cat:STRUCTURE OF GOVERNMENT DEBT> 17  entries\n",
      "<Cat:SUPPLY, USE AND INPUT-OUTPUT TABLES> 37  entries\n",
      "<Cat:SUSTAINABLE DEVELOPMENT GOALS> 31  entries\n",
      "<Cat:TAX REVENUE> 14  entries\n",
      "<Cat:TOURISM COUNTRIES AND REGIONS> 23  entries\n",
      "<Cat:TOURISM INDUSTRIES> 5  entries\n",
      "<Cat:TRADE IN GOODS> 24  entries\n",
      "<Cat:TRADE IN GOODS BY INVOICING CURRENCY> 37  entries\n",
      "<Cat:TRADE IN GOODS BY PARTNER> 29  entries\n",
      "<Cat:TRADE IN GOODS BY PRODUCTS> 47  entries\n",
      "<Cat:TRADE IN SERVICES> 16  entries\n",
      "<Cat:TRANSPORT - ENERGY AND ENVIRONMENT> 4  entries\n",
      "<Cat:TRANSPORT ECONOMY> 1  entries\n",
      "<Cat:TRANSPORT STATISTICS BY AREA AND REGION> 32  entries\n",
      "<Cat:WAGES, EARNINGS AND INCOME> 14  entries\n",
      "<Cat:WASTE> 2  entries\n",
      "<Cat:WORLD TRADE> 2  entries\n",
      "<DAC> 1  entries\n",
      "<DAIMLER> 1  entries\n",
      "<DANISH> 1  entries\n",
      "<DATA> 1  entries\n",
      "<DCFTA> 1  entries\n",
      "<DDA> 1  entries\n",
      "<DE> 4  entries\n",
      "<DE LA LOIRE> 1  entries\n",
      "<DENMARK> 121  entries\n",
      "<DIFFERENT MEMBER STATES> 1  entries\n",
      "<DIGITAL> 3  entries\n",
      "<DIGITAL AGENDA> 1  entries\n",
      "<DIGITALISATION> 1  entries\n",
      "<DIRECTIVES> 2  entries\n",
      "<DMC> 6  entries\n",
      "<DMI> 8  entries\n",
      "<DOLNOSLASKIE> 1  entries\n",
      "<DOMESTIC MATERIAL CONSUMPTION> 1  entries\n",
      "<DRENTHE> 2  entries\n",
      "<DSL> 2  entries\n",
      "<DUBLIN> 6  entries\n",
      "<DUTCH> 7  entries\n",
      "<DÜSSELDORF> 1  entries\n",
      "<EA> 6  entries\n",
      "<EAA> 4  entries\n",
      "<EAC> 3  entries\n",
      "<EASA> 1  entries\n",
      "<EASO> 2  entries\n",
      "<EASTERN> 3  entries\n",
      "<EASTERN POLAND> 1  entries\n",
      "<EC> 33  entries\n",
      "<EC TREATY> 2  entries\n",
      "<ECB> 10  entries\n",
      "<ECDC> 1  entries\n",
      "<ECHI> 3  entries\n",
      "<ECONOMIC> 1  entries\n",
      "<EEA> 8  entries\n",
      "<EEC> 4  entries\n",
      "<EEE> 1  entries\n",
      "<EES> 4  entries\n",
      "<EFA> 1  entries\n",
      "<EFTA> 29  entries\n",
      "<EGSS> 6  entries\n",
      "<EGYPT> 13  entries\n",
      "<EHIS> 19  entries\n",
      "<EHSIS> 1  entries\n",
      "<EIB> 3  entries\n",
      "<ELV> 1  entries\n",
      "<EMA> 1  entries\n",
      "<EMLOYER> 1  entries\n",
      "<EMS> 1  entries\n",
      "<EMU> 3  entries\n",
      "<ENERGY UNION> 1  entries\n",
      "<ENGLISH> 11  entries\n",
      "<ENP> 5  entries\n",
      "<ENTERPRISE> 1  entries\n",
      "<EPE> 2  entries\n",
      "<EPEA> 3  entries\n",
      "<EPRIVACY> 1  entries\n",
      "<EQF> 3  entries\n",
      "<EQUATORIAL GUINEA> 1  entries\n",
      "<ERA> 1  entries\n",
      "<ERASMUS> 3  entries\n",
      "<ERDF> 6  entries\n",
      "<ERP> 1  entries\n",
      "<ESA> 9  entries\n",
      "<ESAW> 3  entries\n",
      "<ESSPROS> 9  entries\n",
      "<ESTONIA> 93  entries\n",
      "<ETEA> 1  entries\n",
      "<ETHIOPIA> 2  entries\n",
      "<ETS> 3  entries\n",
      "<EU> 1447  entries\n",
      "<EU BLUE CARD> 2  entries\n",
      "<EU BLUE CARDS> 1  entries\n",
      "<EU COUNCIL> 1  entries\n",
      "<EU MARCH> 2  entries\n",
      "<EU PRESIDENCIES> 1  entries\n",
      "<EU STATISTICAL REGULATIONS> 1  entries\n",
      "<EUI> 1  entries\n",
      "<EUR> 5  entries\n",
      "<EURATOM> 1  entries\n",
      "<EUROBAROMETER> 3  entries\n",
      "<EUROBASE> 7  entries\n",
      "<EUROMOD> 3  entries\n",
      "<EUROPEAN> 309  entries\n",
      "<EUROPEAN COMMISSION COMMUNICATION> 1  entries\n",
      "<EUROPEAN ENVIRONMENTAL ECONOMIC ACCOUNTS> 2  entries\n",
      "<EUROPEAN INSTITUTIONS> 1  entries\n",
      "<EUROPEAN UNION> 4  entries\n",
      "<EUROPEANS> 24  entries\n",
      "<EUROSTAT> 231  entries\n",
      "<EVAPOTRANSPIRATION> 1  entries\n",
      "<EVRYTANIA> 1  entries\n",
      "<FADN> 5  entries\n",
      "<FADN FADN> 1  entries\n",
      "<FAO> 5  entries\n",
      "<FARMERS> 1  entries\n",
      "<FARMERS AGRICULTURE> 1  entries\n",
      "<FDI> 36  entries\n",
      "<FERTILISERS> 3  entries\n",
      "<FINLAND> 92  entries\n",
      "<FINNISH> 1  entries\n",
      "<FISHER> 2  entries\n",
      "<FLEMISH> 2  entries\n",
      "<FLEVOLAND> 2  entries\n",
      "<FOB> 1  entries\n",
      "<FOCUS> 2  entries\n",
      "<FORESTS> 1  entries\n",
      "<FOUNDATION> 1  entries\n",
      "<FRANCE> 145  entries\n",
      "<FRANKFURT> 1  entries\n",
      "<FRASCATI MANUAL> 3  entries\n",
      "<FRENCH> 24  entries\n",
      "<FSS> 11  entries\n",
      "<FTE> 2  entries\n",
      "<FUERTEVENTURA> 2  entries\n",
      "<GAE> 1  entries\n",
      "<GBAORD> 2  entries\n",
      "<GDP)AND> 1  entries\n",
      "<GEORGIA> 16  entries\n",
      "<GERD> 1  entries\n",
      "<GERMAN> 34  entries\n",
      "<GERMANY> 179  entries\n",
      "<GFCF> 2  entries\n",
      "<GHG> 62  entries\n",
      "<GINI> 10  entries\n",
      "<GIP> 2  entries\n",
      "<GIS> 1  entries\n",
      "<GISCO> 8  entries\n",
      "<GMO> 3  entries\n",
      "<GNI> 4  entries\n",
      "<GNS> 2  entries\n",
      "<GOS> 2  entries\n",
      "<GOVERNMENT> 2  entries\n",
      "<GPG> 6  entries\n",
      "<GRAND COALITION FOR> 1  entries\n",
      "<GREECE> 185  entries\n",
      "<GREEK> 10  entries\n",
      "<GREEN> 2  entries\n",
      "<GREENING> 1  entries\n",
      "<GRONINGEN> 3  entries\n",
      "<GROSS> 4  entries\n",
      "<GRUNDTVIG> 1  entries\n",
      "<GSM> 3  entries\n",
      "<GUADELOUPE> 1  entries\n",
      "<GUYANE> 1  entries\n",
      "<GVA> 4  entries\n",
      "<HAGUE> 1  entries\n",
      "<HAMBURG> 6  entries\n",
      "<HBS> 5  entries\n",
      "<HDI> 2  entries\n",
      "<HEALTHCARE> 4  entries\n",
      "<HELIGOLAND> 1  entries\n",
      "<HELSINKI VANTAA> 1  entries\n",
      "<HEPA> 2  entries\n",
      "<HETUS> 7  entries\n",
      "<HGE> 1  entries\n",
      "<HICP> 10  entries\n",
      "<HICPS> 1  entries\n",
      "<HLY> 2  entries\n",
      "<HONG KONG> 3  entries\n",
      "<HOOFDSTEDELIJK GEWEST> 1  entries\n",
      "<HOUSEHOLDS> 8  entries\n",
      "<HPI> 5  entries\n",
      "<HRST> 4  entries\n",
      "<HRSTO> 4  entries\n",
      "<HUNGARIAN> 2  entries\n",
      "<HUNGARY> 75  entries\n",
      "<IATUR> 4  entries\n",
      "<ICATUS> 1  entries\n",
      "<ICD> 4  entries\n",
      "<ICELAND> 42  entries\n",
      "<ICT> 97  entries\n",
      "<ICW> 1  entries\n",
      "<IDA> 1  entries\n",
      "<IFC> 1  entries\n",
      "<ILO> 1  entries\n",
      "<IMF> 3  entries\n",
      "<IMPR> 1  entries\n",
      "<IMPRO> 5  entries\n",
      "<INDIA> 9  entries\n",
      "<INDONESIA> 5  entries\n",
      "<INSTITUTE> 1  entries\n",
      "<INTRASTAT> 1  entries\n",
      "<INVEST EUROPE> 1  entries\n",
      "<INVOICED> 1  entries\n",
      "<IOI> 2  entries\n",
      "<IPC> 2  entries\n",
      "<IPCC> 1  entries\n",
      "<IPEIROS> 1  entries\n",
      "<IPI> 1  entries\n",
      "<IPM> 1  entries\n",
      "<IRAQ> 1  entries\n",
      "<IRELAND> 147  entries\n",
      "<IRISH> 2  entries\n",
      "<ISCED> 4  entries\n",
      "<ISCO> 2  entries\n",
      "<ISIC> 2  entries\n",
      "<ISRAEL> 20  entries\n",
      "<ITA> 1  entries\n",
      "<ITALIAN> 12  entries\n",
      "<ITALY> 131  entries\n",
      "<ITC> 1  entries\n",
      "<ITGS> 1  entries\n",
      "<ITR> 1  entries\n",
      "<ITSS> 5  entries\n",
      "<ITTA> 1  entries\n",
      "<IUS> 2  entries\n",
      "<IVT> 1  entries\n",
      "<JAMIE> 1  entries\n",
      "<JAPAN> 11  entries\n",
      "<JAPANESE> 2  entries\n",
      "<JERSEY> 1  entries\n",
      "<JFSQ> 1  entries\n",
      "<JOINT> 2  entries\n",
      "<JORDAN> 7  entries\n",
      "<JUNCKER> 1  entries\n",
      "<JVR> 2  entries\n",
      "<KAU> 2  entries\n",
      "<KIA> 1  entries\n",
      "<KIS> 2  entries\n",
      "<KOSOVO> 22  entries\n",
      "<KYOTO> 2  entries\n",
      "<KÖLN> 2  entries\n",
      "<LANGUAGE> 1  entries\n",
      "<LAOS> 1  entries\n",
      "<LASPEYRES> 4  entries\n",
      "<LATIN> 1  entries\n",
      "<LATVIA> 96  entries\n",
      "<LATVIAN> 1  entries\n",
      "<LATVIJA> 1  entries\n",
      "<LAU> 1  entries\n",
      "<LCI> 3  entries\n",
      "<LCS> 2  entries\n",
      "<LEBANON> 9  entries\n",
      "<LEONTIEF> 1  entries\n",
      "<LEVADAS> 1  entries\n",
      "<LFS> 7  entries\n",
      "<LICHTENSTEIN> 1  entries\n",
      "<LIECHTENSTEIN> 5  entries\n",
      "<LIETUVA> 1  entries\n",
      "<LIGURIA> 2  entries\n",
      "<LISBON> 4  entries\n",
      "<LITHIANIA> 1  entries\n",
      "<LITHUANIA> 100  entries\n",
      "<LITHUANIAN> 4  entries\n",
      "<LITHUANIANS> 1  entries\n",
      "<LIVESTOCK> 1  entries\n",
      "<LIVORNO> 1  entries\n",
      "<LKIS> 1  entries\n",
      "<LMP> 4  entries\n",
      "<LODI> 1  entries\n",
      "<LOMBARDIA> 7  entries\n",
      "<LOWESTOFT> 1  entries\n",
      "<LPG> 2  entries\n",
      "<LSU> 3  entries\n",
      "<LU> 2  entries\n",
      "<LUCAS> 4  entries\n",
      "<LULUCF> 8  entries\n",
      "<LUXEMBOURG> 169  entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LUXEMBOURGIAN> 1  entries\n",
      "<LUXEMBOURGISH> 1  entries\n",
      "<MAASTRICHT> 5  entries\n",
      "<MACINTOSH> 1  entries\n",
      "<MADEIRA> 3  entries\n",
      "<MADRID> 2  entries\n",
      "<MALAYSIA> 3  entries\n",
      "<MALTA> 122  entries\n",
      "<MALTESE> 2  entries\n",
      "<MANAGEMENT BOARD> 1  entries\n",
      "<MARSEILLE> 2  entries\n",
      "<MARTINIQUE> 1  entries\n",
      "<MAYOTTE> 5  entries\n",
      "<MBT> 1  entries\n",
      "<MELILLA> 1  entries\n",
      "<MEMBER STATES> 65  entries\n",
      "<MERCOSUR> 1  entries\n",
      "<MESSINA> 1  entries\n",
      "<MEXICO> 3  entries\n",
      "<MICROBLOGGING> 1  entries\n",
      "<MILAN> 1  entries\n",
      "<MIP> 1  entries\n",
      "<MMTCDE> 1  entries\n",
      "<MNC> 1  entries\n",
      "<MNE> 1  entries\n",
      "<MOLDOVA> 13  entries\n",
      "<MONGOLIA> 1  entries\n",
      "<MONTENEGRO> 29  entries\n",
      "<MOROCCAN> 1  entries\n",
      "<MOROCCANS> 1  entries\n",
      "<MOROCCO> 20  entries\n",
      "<MOX> 1  entries\n",
      "<MPI> 2  entries\n",
      "<MSITS> 1  entries\n",
      "<MSY> 1  entries\n",
      "<MUNICH> 2  entries\n",
      "<MUNICH REINSURANCE COMPANY> 1  entries\n",
      "<MYANMAR> 4  entries\n",
      "<NACE> 26  entries\n",
      "<NACE DIVISIONS> 1  entries\n",
      "<NACE SECTIONS> 1  entries\n",
      "<NATIONAL ACTION PLANS> 1  entries\n",
      "<NATIONAL STATISTICAL INSTITUTES> 3  entries\n",
      "<NATURA> 5  entries\n",
      "<NB> 1  entries\n",
      "<NDP> 1  entries\n",
      "<NEC> 1  entries\n",
      "<NEEP> 1  entries\n",
      "<NEET> 23  entries\n",
      "<NEET STATISTICS> 2  entries\n",
      "<NESA> 7  entries\n",
      "<NETHERLANDS> 125  entries\n",
      "<NEW ZEALAND> 2  entries\n",
      "<NEXT GENERATION EU> 1  entries\n",
      "<NIIP> 2  entries\n",
      "<NIS> 1  entries\n",
      "<NITROGEN> 3  entries\n",
      "<NMP> 1  entries\n",
      "<NORDIC> 4  entries\n",
      "<NORRA SVERIGE> 1  entries\n",
      "<NORTE> 1  entries\n",
      "<NORTH MACEDONIA> 1  entries\n",
      "<NORWAY> 46  entries\n",
      "<NORWEGIAN> 3  entries\n",
      "<NOTIO AIGAIO> 2  entries\n",
      "<NOX> 1  entries\n",
      "<NPISH> 2  entries\n",
      "<NUAA> 1  entries\n",
      "<NUE> 2  entries\n",
      "<NURSERIES> 1  entries\n",
      "<NUTRIENTS> 2  entries\n",
      "<OBERBAYERN> 3  entries\n",
      "<OBESITY> 1  entries\n",
      "<ODA> 10  entries\n",
      "<OECD> 9  entries\n",
      "<OFATS> 1  entries\n",
      "<OILSEEDS FIBRE> 1  entries\n",
      "<OMC> 4  entries\n",
      "<OPEN WORKING GROUP> 1  entries\n",
      "<ORIO AL SERIO> 1  entries\n",
      "<ORLY> 1  entries\n",
      "<OVERIG GRONINGEN> 1  entries\n",
      "<PAASCHE> 2  entries\n",
      "<PAKISTAN> 5  entries\n",
      "<PALESTINE> 16  entries\n",
      "<PARIS> 7  entries\n",
      "<PARLIAMENT> 2  entries\n",
      "<PASSAGEWAYS> 1  entries\n",
      "<PAYG> 5  entries\n",
      "<PDO> 6  entries\n",
      "<PEEI> 4  entries\n",
      "<PEFA> 2  entries\n",
      "<PELOPONNISOS> 1  entries\n",
      "<PERU> 1  entries\n",
      "<PES> 1  entries\n",
      "<PH> 3  entries\n",
      "<PHILIPPINES> 2  entries\n",
      "<PILLAR> 1  entries\n",
      "<PIRAEUS> 1  entries\n",
      "<PISA> 1  entries\n",
      "<PLI> 2  entries\n",
      "<POLAND> 90  entries\n",
      "<POLISH> 6  entries\n",
      "<PORTUGAL> 73  entries\n",
      "<PORTUGUESE> 4  entries\n",
      "<PPP> 5  entries\n",
      "<PPS> 27  entries\n",
      "<PRAGUE> 2  entries\n",
      "<PRODCOM> 3  entries\n",
      "<PROTOCOL> 4  entries\n",
      "<PROV> 1  entries\n",
      "<PTB> 1  entries\n",
      "<QOL> 1  entries\n",
      "<REGIÃO AUTÓNOMA> 1  entries\n",
      "<REGIÕES> 1  entries\n",
      "<REGULATION> 12  entries\n",
      "<REPROTOXIC> 1  entries\n",
      "<RFID> 1  entries\n",
      "<RHINE> 1  entries\n",
      "<RMC> 7  entries\n",
      "<RMI> 1  entries\n",
      "<ROMA> 1  entries\n",
      "<ROMANIA> 162  entries\n",
      "<ROMANIAN> 3  entries\n",
      "<ROMANIANS> 1  entries\n",
      "<ROTTERDAM> 13  entries\n",
      "<ROUNDWOOD> 1  entries\n",
      "<ROW> 2  entries\n",
      "<RUSSIA> 7  entries\n",
      "<RUSSIAN> 3  entries\n",
      "<S&T> 8  entries\n",
      "<SALZBURG> 3  entries\n",
      "<SAMOS> 1  entries\n",
      "<SANDERS> 1  entries\n",
      "<SANKEY> 9  entries\n",
      "<SAPM> 9  entries\n",
      "<SBA> 1  entries\n",
      "<SBS> 12  entries\n",
      "<SCHENGEN> 3  entries\n",
      "<SDG> 23  entries\n",
      "<SDMX> 1  entries\n",
      "<SEPA> 2  entries\n",
      "<SERBIA> 40  entries\n",
      "<SERBIAN> 1  entries\n",
      "<SES> 8  entries\n",
      "<SEVEROZAPADEN> 2  entries\n",
      "<SEVEROZÁPAD> 2  entries\n",
      "<SGM> 7  entries\n",
      "<SGP> 3  entries\n",
      "<SHA> 4  entries\n",
      "<SICILIA> 2  entries\n",
      "<SINGAPORE> 13  entries\n",
      "<SITC> 2  entries\n",
      "<SKILLS> 1  entries\n",
      "<SLASKIE> 1  entries\n",
      "<SLATTED> 1  entries\n",
      "<SLOVAKIA> 61  entries\n",
      "<SLOVAKIAN> 2  entries\n",
      "<SLOVENIA> 61  entries\n",
      "<SLOVENIA LUXEMBOURG> 1  entries\n",
      "<SLOVENIAN> 2  entries\n",
      "<SLOVENIANS> 1  entries\n",
      "<SME> 6  entries\n",
      "<SNA> 2  entries\n",
      "<SOIL THEMATIC STRATEGY> 1  entries\n",
      "<SOUTH AFRICA> 4  entries\n",
      "<SOUTH KOREA> 1  entries\n",
      "<SOUTHERN> 3  entries\n",
      "<SPAIN> 129  entries\n",
      "<SPANISH> 23  entries\n",
      "<SPE> 1  entries\n",
      "<SPECIAL PROTECTION AREAS> 1  entries\n",
      "<SPORT SATELLITE ACCOUNTS> 1  entries\n",
      "<SPPI> 5  entries\n",
      "<SPPIS> 1  entries\n",
      "<SPRINKLER> 1  entries\n",
      "<SSS> 1  entries\n",
      "<STABILITY> 1  entries\n",
      "<STATE> 4  entries\n",
      "<STATES> 21  entries\n",
      "<STATISTICS> 2  entries\n",
      "<STEC> 12  entries\n",
      "<STI> 1  entries\n",
      "<STOCKHOLM> 3  entries\n",
      "<STRASBOURG> 1  entries\n",
      "<STRUCTURAL BUSINESS STATISTICS> 1  entries\n",
      "<STS> 7  entries\n",
      "<STS REGULATION> 1  entries\n",
      "<STUTTGART> 3  entries\n",
      "<SUB)POPULATION> 1  entries\n",
      "<SUD> 1  entries\n",
      "<SULLIVAN> 4  entries\n",
      "<SULLIVAN HEALTH EXPECTANCY> 1  entries\n",
      "<SWEDEN> 90  entries\n",
      "<SWEDISH> 2  entries\n",
      "<SWISS> 2  entries\n",
      "<SWITZERLAND> 32  entries\n",
      "<SYRIA> 4  entries\n",
      "<SYRIAN> 4  entries\n",
      "<SYRIANS> 3  entries\n",
      "<SYSTEM> 1  entries\n",
      "<TBP> 1  entries\n",
      "<TDMA> 1  entries\n",
      "<TEC> 3  entries\n",
      "<TEGEL> 1  entries\n",
      "<THAILAND> 7  entries\n",
      "<THE ACTION PROGRAMMES> 1  entries\n",
      "<THE ADMINISTRATIVE COUNCIL> 1  entries\n",
      "<THE AFRICAN HEALTH STRATEGY> 1  entries\n",
      "<THE BIRDS DIRECTIVE> 1  entries\n",
      "<THE COMMUNICATION MULTILINGUALISM> 1  entries\n",
      "<THE COMPETITIVENESS COUNCIL> 3  entries\n",
      "<THE COUNCIL> 1  entries\n",
      "<THE COUNCIL CONCLUSIONS> 1  entries\n",
      "<THE COUNCIL REGULATION> 1  entries\n",
      "<THE CZECH REPUBLIC> 2  entries\n",
      "<THE DIGITAL SINGLE MARKET> 1  entries\n",
      "<THE DOHA AMENDMENT> 1  entries\n",
      "<THE DOHA DEVELOPMENT AGENDA> 1  entries\n",
      "<THE DUBLIN REGULATION> 1  entries\n",
      "<THE EC TREATY> 1  entries\n",
      "<THE EEC TREATY> 2  entries\n",
      "<THE ENERGY UNION> 1  entries\n",
      "<THE EU ADAPTATION STRATEGY> 1  entries\n",
      "<THE EU BIODIVERSITY STRATEGY> 1  entries\n",
      "<THE EU BIRDS> 1  entries\n",
      "<THE EU GUIDELINES> 1  entries\n",
      "<THE EU PACT> 1  entries\n",
      "<THE EU TREATY> 1  entries\n",
      "<THE EUROPEAN CENTRAL BANK> 2  entries\n",
      "<THE EUROPEAN CHEMICALS AGENCY> 1  entries\n",
      "<THE EUROPEAN COMMISSION> 231  entries\n",
      "<THE EUROPEAN COMMUNITY> 2  entries\n",
      "<THE EUROPEAN CORE HEALTH INDICATOR> 1  entries\n",
      "<THE EUROPEAN CORE HEALTH INDICATORS> 1  entries\n",
      "<THE EUROPEAN COUNCIL> 6  entries\n",
      "<THE EUROPEAN DIGITAL STRATEGY> 1  entries\n",
      "<THE EUROPEAN ECONOMIC COMMUNITY> 1  entries\n",
      "<THE EUROPEAN ENVIRONMENT AGENCY> 6  entries\n",
      "<THE EUROPEAN FOREST ACCOUNTS> 1  entries\n",
      "<THE EUROPEAN FOUNDATION> 1  entries\n",
      "<THE EUROPEAN GREEN DEAL> 19  entries\n",
      "<THE EUROPEAN GREEN DEAL ’> 1  entries\n",
      "<THE EUROPEAN MEDICINES AGENCY> 1  entries\n",
      "<THE EUROPEAN MIGRATION NETWORK> 1  entries\n",
      "<THE EUROPEAN MONETARY SYSTEM> 1  entries\n",
      "<THE EUROPEAN PACT> 1  entries\n",
      "<THE EUROPEAN PARLIAMENT> 14  entries\n",
      "<THE EUROPEAN PATENT CONVENTION> 2  entries\n",
      "<THE EUROPEAN PATENT OFFICE> 2  entries\n",
      "<THE EUROPEAN REGIONAL DEVELOPMENT FUND> 3  entries\n",
      "<THE EUROPEAN RESEARCH AREA> 4  entries\n",
      "<THE EUROPEAN SEMESTER> 3  entries\n",
      "<THE EUROPEAN STANDARD POPULATION> 1  entries\n",
      "<THE EUROPEAN STATISTICAL SYSTEM> 3  entries\n",
      "<THE EUROPEAN STRUCTURE> 1  entries\n",
      "<THE EUROPEAN UNION> 17  entries\n",
      "<THE EUROSTAT DECISION> 1  entries\n",
      "<THE EU‘S SUSTAINABLE DEVELOPMENT STRATEGY> 1  entries\n",
      "<THE EXCHANGE RATE MECHANISM> 1  entries\n",
      "<THE FIRST WORLD WAR> 1  entries\n",
      "<THE FOREST LAW ENFORCEMENT> 1  entries\n",
      "<THE INTERNATIONAL FINANCE CORPORATION> 1  entries\n",
      "<THE INTERNATIONAL LABOUR ORGANISATION> 2  entries\n",
      "<THE INTERNATIONAL MONETARY FUND> 2  entries\n",
      "<THE JOINT ACTION> 1  entries\n",
      "<THE KYOTO PROTOCOL> 3  entries\n",
      "<THE LISBON TREATY> 2  entries\n",
      "<THE LJUBLJANA PROCESS> 1  entries\n",
      "<THE MEMBER STATE> 1  entries\n",
      "<THE MEMBER STATES> 16  entries\n",
      "<THE MONTREAL PROTOCOL> 1  entries\n",
      "<THE MUNICH DIPLOMATIC CONFERENCE> 1  entries\n",
      "<THE NATURE ACTION PLAN> 1  entries\n",
      "<THE NETHERLANDS> 1  entries\n",
      "<THE NETHERLANDS (> 1  entries\n",
      "<THE PARIS AGREEMENT> 3  entries\n",
      "<THE PRODCOM REGULATION> 1  entries\n",
      "<THE RUSSIAN FEDERATION> 3  entries\n",
      "<THE SCHENGEN BORDERS CODE> 1  entries\n",
      "<THE SINGLE MARKET> 1  entries\n",
      "<THE STRASBOURG AGREEMENT> 1  entries\n",
      "<THE SUSTAINABLE DEVELOPMENT GOALS> 1  entries\n",
      "<THE UN CONVENTION> 2  entries\n",
      "<THE UN DEVELOPMENT PROGRAMME> 1  entries\n",
      "<THE UNITED ARAB EMIRATES> 1  entries\n",
      "<THE UNITED KINGDOM> 41  entries\n",
      "<THE UNITED NATIONS> 8  entries\n",
      "<THE UNITED STATES> 28  entries\n",
      "<THE VOLKSWAGEN GROUP> 1  entries\n",
      "<THE VON DER LEYEN COMMISSION> 1  entries\n",
      "<THE WHITE PAPER> 1  entries\n",
      "<THE WORLD BANK> 3  entries\n",
      "<THE WORLD HEALTH ORGANISATION> 9  entries\n",
      "<THE WORLD HEALTH ORGANIZATION> 1  entries\n",
      "<THE WORLD TRADE ORGANIZATION> 4  entries\n",
      "<THE WORLD WIDE WEB> 1  entries\n",
      "<THE ZARAGOZA DECLARATION> 1  entries\n",
      "<THE ÎLE DE FRANCE> 2  entries\n",
      "<THE ‘ BLUEPRINT> 1  entries\n",
      "<THEDEFINITION> 1  entries\n",
      "<THEINDEX> 1  entries\n",
      "<THESPROTIA> 1  entries\n",
      "<THÜRINGEN> 1  entries\n",
      "<TINED> 1  entries\n",
      "<TISA> 2  entries\n",
      "<TITLE VI> 1  entries\n",
      "<TMT> 2  entries\n",
      "<TPP> 2  entries\n",
      "<TRAILER> 1  entries\n",
      "<TRANSPARENCY INTERNATIONAL> 1  entries\n",
      "<TREATIES> 2  entries\n",
      "<TREATY> 4  entries\n",
      "<TREBBIANO> 1  entries\n",
      "<TREE> 3  entries\n",
      "<TREE DIFFERENCES> 1  entries\n",
      "<TUNISIA> 6  entries\n",
      "<TURKEY> 94  entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TURKISH> 2  entries\n",
      "<UAA> 10  entries\n",
      "<UCI> 1  entries\n",
      "<UK> 7  entries\n",
      "<UKRAINE> 18  entries\n",
      "<UKRAINIAN> 3  entries\n",
      "<UKRAINIANS> 2  entries\n",
      "<ULTIMATE> 1  entries\n",
      "<UN> 21  entries\n",
      "<UNECE> 1  entries\n",
      "<UNEMPLOYED> 1  entries\n",
      "<UNESCO> 3  entries\n",
      "<UNESCO S&T> 1  entries\n",
      "<UNFCCC> 1  entries\n",
      "<UNION> 1  entries\n",
      "<UNITED STATES> 1  entries\n",
      "<UNSC> 1  entries\n",
      "<UNWTO> 4  entries\n",
      "<UR> 2  entries\n",
      "<URBAN> 2  entries\n",
      "<US> 8  entries\n",
      "<USA> 1  entries\n",
      "<USP> 1  entries\n",
      "<UTRECHT> 3  entries\n",
      "<UWWTP> 1  entries\n",
      "<VALENCIA> 1  entries\n",
      "<VENICE> 1  entries\n",
      "<VET> 4  entries\n",
      "<VIETNAM> 4  entries\n",
      "<VLAAMS GEWEST> 1  entries\n",
      "<VOLKSWAGEN> 1  entries\n",
      "<VÝCHODNÉ SLOVENSKO> 1  entries\n",
      "<WA> 2  entries\n",
      "<WARSZAWSKI> 2  entries\n",
      "<WATERWAYS> 1  entries\n",
      "<WHITE> 4  entries\n",
      "<WHITE PAPER> 2  entries\n",
      "<WLTP> 1  entries\n",
      "<WOLFSBURG> 1  entries\n",
      "<WORLD WAR II> 1  entries\n",
      "<WTO> 7  entries\n",
      "<XG ECO> 1  entries\n",
      "<YEI> 1  entries\n",
      "<YOUTH> 4  entries\n",
      "<YUGOZAPADEN> 1  entries\n",
      "<YUZHEN> 1  entries\n",
      "<ZARAGOZA> 6  entries\n",
      "<ZEEBRUGGE> 1  entries\n",
      "<ZEELAND> 1  entries\n",
      "<•TO> 2  entries\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "#import pickle\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "def file_name(pre,ext):\n",
    "    current_time = datetime.datetime.now() \n",
    "    return pre + '_'+ str(current_time.month)+ '_' + str(current_time.day) + \\\n",
    "                 '_' + str(current_time.hour)+ '_' + str(current_time.minute)  +'.'+ext\n",
    "    \n",
    "outfile3 = file_name('SVOs_all','txt')\n",
    "outfile3b = file_name('SVOs_all','pkl')\n",
    "outfile3c = file_name('SVOs_all','xlsx')\n",
    "\n",
    "#with open(outfile3b, 'wb') as file:\n",
    "#        pickle.dump(Glob_NER_SVOs, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "Glob_NER_SVOs_2 = {k:v for k,v in sorted(Glob_NER_SVOs.items(), key=lambda item: item[0])}\n",
    "\n",
    "results = pd.DataFrame(index=range(len(Glob_NER_SVOs.items())),columns=['Key','Source','Subject','Verb','Object','Title','URL','Sentence'])\n",
    "c = -1\n",
    "with open(outfile3, 'w') as file:\n",
    "    for key in Glob_NER_SVOs_2.keys():\n",
    "            print('<'+key+'>',end=' ')\n",
    "            number = Glob_NER_SVOs_2[key][5]\n",
    "            print(number, ' entries')\n",
    "            #Glob_NER_SVOs[key][0].append(sv) \n",
    "            #Glob_NER_SVOs[key][1].append(source) \n",
    "            #Glob_NER_SVOs[key][2].append(dat.loc[i,'title'])\n",
    "            #Glob_NER_SVOs[key][3].append(dat.loc[i,'url'])\n",
    "            #Glob_NER_SVOs[key][4].append(sent.text)                                \n",
    "            #Glob_NER_SVOs[key][5] += 1    \n",
    "            phrases, sources, titles, urls, sentences = Glob_NER_SVOs_2[key][0:5]\n",
    "            for (i,(phrase,source,title,url,sentence)) in enumerate(zip(phrases,sources,titles,urls,sentences)):\n",
    "                s = unidecode.unidecode(str(phrase))\n",
    "                s0 = unidecode.unidecode(phrase[0])\n",
    "                s1 = unidecode.unidecode(phrase[1])\n",
    "                s2 = unidecode.unidecode(phrase[2])\n",
    "                st = unidecode.unidecode(title)\n",
    "                surl=url\n",
    "                ss = unidecode.unidecode(sentence)\n",
    "                ## print('{0:70s} / {1:30s} {2:5d}: {3:s}\\n'.format(key,source,i,s))\n",
    "                ##print(ss)\n",
    "                file.write('{0:70s} / {1:30s} {2:5d}: {3:30s} {4:30s} {5:30s} {6:s} {7:s}\\n'.format(unidecode.unidecode(key),source,i,s0,s1,s2,st,ss))\n",
    "                #file.write('{0:40s} / {1:16s} {2:4d}: {3:s}\\n'.format(unidecode.unidecode(key),source,i,s))\n",
    "                c +=1\n",
    "                results.loc[c,'Key'] = format(unidecode.unidecode(key))\n",
    "                results.loc[c,'Source'] = source\n",
    "                results.loc[c,'Subject'] = s0\n",
    "                results.loc[c,'Verb'] = s1\n",
    "                results.loc[c,'Object'] = s2\n",
    "                results.loc[c,'Title'] = st\n",
    "                results.loc[c,'URL'] = surl\n",
    "                results.loc[c,'Sentence'] = ss\n",
    "\n",
    "#results.to_excel(outfile3c)\n",
    "results.to_excel('SVOs_all.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verify the information written to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with open(outfile3c, 'r') as f:\n",
    "    count = 0\n",
    " \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

 ### Use Case C
 - [Use Case C Word embeddings](https://github.com/eurostat/NLP4Stat/tree/testing/Use%20case%20C/Use%20Case%20C%20Word%20embeddings). Word vectors trained on SE articles and SE Glossary articles. Application for the identification of Eurostat datasets. The processing of SE and SE Glossary articles is for the first run only, to save the vectors model. After this, it suffices to load the model. This is also saved in plain text format for inspection (see file SE_GL_wordvectors.txt in folder). Requires the file _table_of_contents.xml_ in the same folder as the notebook.
 - [Topic Modelling_and_Word_embeddings_v2.ipynb](https://github.com/eurostat/NLP4Stat/blob/testing/Use%20case%20C/Topic%20Modelling_and_Word_embeddings_v2.ipynb). Combination of topic modelling and word embeddings for the identification of statistical datasets. Requires the file _table_of_contents.xml_ in the same folder as the notebook. Also, the saved LDA model (_lda_model.pl_, also in the same folder, see file contained in compressed file [lda_model.rar](https://github.com/eurostat/NLP4Stat/blob/testing/Use%20case%20C/lda_model.rar)), if this model is **not** re-created (see comments in code).
    - Carry-out topic modelling with a large enough corpus (Statistics Explained articles and Statistics Explained Glossary articles) and a large number of topics (1000) and extract significant (lemmatized) keywords. The objectives are two:
        - to cover the whole corpus and thus the "correlated" datasets at a high granularity,
        - avoid using common ("dominating") words in the matches with the user's query.
     - Enhance these keywords with their closest terms from the word embeddings created exclusively from Eurostat's content. The total large number of keywords can then differentiate the datasets.
     - Match the (similarly enhanced) sentence(s) entered in the query, with datasets, based on the number of keywords found in the datasets (simple or full descriptions).
     - Put first priority to the matches with words in the enhanced topic modelling dictionary and second to the matches with any other words, to avoid "dominating" terms.
     - The union of the topic modelling keywords found in the datasets descriptions (enhanced or not) can also be used as multi-labels in a multi-label classification algorithm such as BERT.
     - The data with the multi-labels are optionally produced in this code.
 - [BERT model Use Case C v3.ipynb](https://github.com/eurostat/NLP4Stat/blob/testing/Use%20case%20C/BERT%20model%20Use%20Case%20C%20v3.ipynb). A BERT model, based on the SentenceTransformers Python framework  which uses the BERT algorithm to compute sentence embeddings. The strategy employed is to use a pre-trained BERT model, fine-tune it to the available corpus, and then use the “retrieve & re-rank pipeline” approach for the ranking of the matches, as is suggested for complex semantic search scenarios. This is a  **Google Colab** notebook and requires GPU for proper running. It also requires setting-up a Google drive to read the file _table_of_contents.xml_. Alternatively, this file can be uploaded on the spot. The model can be saved once to the Google drive and re-loaded to avoid all heavy computations steps.

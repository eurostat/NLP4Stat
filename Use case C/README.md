 ### Use Case C
 - [Use Case C Word embeddings](https://github.com/eurostat/NLP4Stat/tree/testing/Use%20case%20C/Use%20Case%20C%20Word%20embeddings). Word vectors trained on SE articles and SE Glossary articles. Application for the identification of Eurostat datasets. The processing of SE and SE Glossary articles is for the first run only, to save the vectors model. After this, it suffices to load the model. This is also saved in plain text format for inspection (see file SE_GL_wordvectors.txt in folder). Revised (February 2022), no more requires the external file _table_of_contents.xml_.
 - [Use Case C Topic modelling and Word embeddings](https://github.com/eurostat/NLP4Stat/tree/testing/Use%20case%20C/Use%20Case%20C%20Topic%20modelling%20and%20Word%20embeddings). Combination of topic modelling and word embeddings for the identification of statistical datasets. Revised (February 2022), no more requires the external file _table_of_contents.xml_. One can either re-create the LDA model or load the saved one from the previous code, from file _lda_model.pl_ contained in compressed file lda_model.rar. A copy is included in the /Data folder. Main features:
    - Carry-out topic modelling with a large enough corpus (Statistics Explained articles and Statistics Explained Glossary articles) and a large number of topics (1000) and extract significant (lemmatized) keywords. The objectives are two:
        - to cover the whole corpus and thus the "correlated" datasets at a high granularity,
        - avoid using common ("dominating") words in the matches with the user's query.
     - Enhance these keywords with their closest terms from the word embeddings created exclusively from Eurostat's content. The total large number of keywords can then differentiate the datasets.
     - Match the (similarly enhanced) sentence(s) entered in the query, with datasets, based on the number of keywords found in the datasets (simple or full descriptions).
     - Put first priority to the matches with words in the enhanced topic modelling dictionary and second to the matches with any other words, to avoid "dominating" terms.
     - The union of the topic modelling keywords found in the datasets descriptions (enhanced or not) can also be used as multi-labels in a multi-label classification algorithm such as BERT.
     - The data with the multi-labels are optionally produced in this code.
 - [Use Case C BERT model](https://github.com/eurostat/NLP4Stat/tree/testing/Use%20case%20C/Use%20Case%20C%20BERT%20model). A BERT model, based on the SentenceTransformers Python framework  which uses the BERT algorithm to compute sentence embeddings. The strategy employed is to use a pre-trained BERT model, fine-tune it to the available corpus, and then use the “retrieve & re-rank pipeline” approach for the ranking of the matches, as is suggested for complex semantic search scenarios. This is a  **Google Colab** notebook and requires GPU for proper running. It also requires setting-up a Google drive to store the model and retrieve it in re-runs, avoiding the long computation time it requires.

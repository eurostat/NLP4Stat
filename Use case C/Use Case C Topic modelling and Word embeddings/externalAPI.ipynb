{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"externalAPI.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install nltk==3.4 ## needs latest version\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOf21a1l46oF","outputId":"de815c0d-4db5-483e-e4d3-2c054f306b3c","executionInfo":{"status":"ok","timestamp":1654602258529,"user_tz":-180,"elapsed":2993,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.4 in /usr/local/lib/python3.7/dist-packages (3.4)\n","Requirement already satisfied: singledispatch in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (3.7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYI_gObD0lem","outputId":"0d3e8067-84cb-4ce7-b7bf-28b9c0eab601","executionInfo":{"status":"ok","timestamp":1654602259199,"user_tz":-180,"elapsed":680,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import requests\n","import re\n","\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","#from nltk.tokenize import word_tokenize\n","from nltk.corpus import wordnet as wn\n","\n","\n","\n"]},{"cell_type":"code","source":["def Remove(duplicate):\n","    final = []\n","    for num in duplicate:\n","        if num not in final:\n","            final.append(num)\n","    return final"],"metadata":{"id":"14sFGrY412M-","executionInfo":{"status":"ok","timestamp":1654602259200,"user_tz":-180,"elapsed":11,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def wordNet(word):\n","  keyword = word.split(\" \")\n","\n","  listWN = []\n","  for key in keyword:\n","    listKeyword = []\n","    synonyms = wn.synsets(key)\n","    for s in synonyms:\n","      helper = s.name().replace(\".n.\", \"\").replace(\".v.\", \"\").replace(\".r.\", \"\").replace(\".a.\", \"\")\n","      result = ''.join([i for i in helper if not i.isdigit()])\n","      if result not in keyword:\n","        syn1 = wn.synsets(key)[0]\n","        syn2 = wn.synsets(result)[0]\n","        score = syn1.wup_similarity(syn2)\n","        listKeyword.append((result, score))\n","    listWN.append((key, listKeyword))\n","\n","  finalListWN =[]\n","  for l in listWN:\n","    finalListWN.append((l[0], Remove(l[1])))\n","\n","\n","  return finalListWN"],"metadata":{"id":"pVQNd2Q01wFo","executionInfo":{"status":"ok","timestamp":1654602259201,"user_tz":-180,"elapsed":9,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class findSimilarity():\n","\n","    def __init__(self, entities, properties, perceived, weights):\n","        self.entities = entities\n","        self.properties = properties\n","        self.perceived = perceived\n","        self.weights = weights\n","        \n","    def Remove(self, duplicate):\n","        final = []\n","        for num in duplicate:\n","            if num not in final:\n","                final.append(num)\n","        return final\n","\n","    def hasNumbers(self, inputString):\n","        return any(char.isdigit() for char in inputString)\n","\n","    def cleaning_entities(self):\n","        list1 = []\n","        entity_property, second_cleaning = {}, []\n","        for i in range(len(self.entities)):\n","            first_cleaning = self.entities[i].replace('/a/[', '').replace(']', '')\n","            for j in range(len(self.properties)):\n","                if self.properties[j] + '/' in first_cleaning:\n","                    second_cleaning = first_cleaning.replace(self.properties[j] + '/', '').replace(',', '')\n","                    if entity_property.get(self.properties[j]) == None:\n","                        entity_property.setdefault(self.properties[j], []).append(second_cleaning)\n","                    else:\n","                        entity_property[self.properties[j]].append(second_cleaning)\n","                    break\n","            list1.append((second_cleaning, self.weights[i]))\n","\n","\n","        wanted_properties = [\"/r/RelatedTo\", \"/r/Synonym\"]\n","\n","        list_key_help = []\n","        for key in entity_property.keys():\n","            if key in wanted_properties:\n","                list_key_help.append(key)\n","\n","        entity_property_keep = {}\n","        for property in wanted_properties:\n","            if property in list_key_help:\n","                entity_property_keep[property] = entity_property[property]\n","            else:\n","                entity_property_keep[property] = []\n","\n","\n","        return list1, entity_property_keep\n","\n","    def cleaning_entities2(self, entities_observed, perceived_weights):\n","\n","        for observed in entities_observed:\n","            for entity in range(len(entities_observed[observed]) - 1, -1, -1):\n","                if not (entities_observed[observed][entity].count(\"/en/\") == 2):\n","                    del entities_observed[observed][entity]\n","                else:\n","                    for perceived_entity in self.perceived:\n","                        if (('/c/en/' + perceived_entity + '/') in entities_observed[observed][entity]):\n","                            entities_observed[observed][entity] = entities_observed[observed][entity].replace(\n","                                '/c/en/' + perceived_entity + '/', '')\n","                        if (('/c/en' + perceived_entity + '/n/') in entities_observed[observed][entity]):\n","                            entities_observed[observed][entity] = entities_observed[observed][entity].replace(\n","                                '/c/en/' + perceived_entity + '/n/', '')\n","\n","        for observed in entities_observed:\n","            annotator = 0\n","            for i in entities_observed[observed]:\n","                if ('wn/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('wn/', '_')\n","                if ('/n/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('/n/', '')\n","                if ('/v/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('/v/', '')\n","                if ('/a/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('/a/', '')\n","                if ('/c/en/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('/c/en/',\n","                                                                                                            '')\n","                if ('n/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('n/', '')\n","                if ('/' in i):\n","                    entities_observed[observed][annotator] = entities_observed[observed][annotator].replace('/', '')\n","                annotator += 1\n","\n","        for observed in entities_observed:\n","            entities_observed[observed] = self.Remove(entities_observed[observed])\n","            for i in entities_observed[observed]:\n","                if self.hasNumbers(i) == True:\n","                    entities_observed[observed].remove(i)\n","\n","        helper_list_weights = []\n","        for property in entities_observed:\n","            for entity in entities_observed[property]:\n","                helper_list_weights.append(entity)\n","\n","        list_with_weights = []\n","        for entity in helper_list_weights:\n","            for weight in perceived_weights:\n","                if entity in weight[0]:\n","                    list_with_weights.append((entity, weight[1]))\n","\n","        return entities_observed, list_with_weights\n","\n","    def grounding(self, cleaned_entities, weigths_of_entities):\n","\n","        porter = PorterStemmer()\n","        weigths_of_entities = self.Remove(weigths_of_entities)\n","        helperWeights = []\n","        for w in weigths_of_entities:\n","          if w[1] > 1.0 and \"_\" not in w[0] and str(w[0]) != str(self.perceived[0]):\n","            relation = \"\"\n","            for prp in cleaned_entities:\n","              if w[0] in cleaned_entities[prp]:\n","                realtion = str(prp)\n","                break\n","            helperWeights.append((w[0], w[1], prp.replace(\"/r/\", \"\")))\n","       \n","        visited = set()\n","\n","        Output = []\n","          \n","        # Iteration\n","        for a, b, c in helperWeights:\n","            if not a in visited:\n","                visited.add(a)\n","                Output.append((a, b, c))\n","\n","        helperNew = []\n","        for h in Output:\n","            syn = wn.synsets(str(h[0]))\n","            if len(syn) > 1:\n","                helperNew.append(h)\n","        helperNew = sorted(helperNew, key = lambda x: x[1], reverse=True)\n","        return helperNew "],"metadata":{"id":"wUj0Pvv7w56r","executionInfo":{"status":"ok","timestamp":1654602259576,"user_tz":-180,"elapsed":383,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def myfunc(term):\n","    return 'http://api.conceptnet.io/c/en/' + term + '?offset=0&limit=1000'\n","\n","\n","def helperCN(c):\n","\n","    id = myfunc(c)\n","    respone = requests.get(id)\n","    objAction = respone.json()\n","    lst1 = [relation['rel']['@id'] for relation in objAction['edges']]\n","    lst2 = [relation['@id'] for relation in objAction['edges']]\n","    lst3 = [relation['weight'] for relation in objAction['edges']]\n","\n","    return lst1, lst2, lst3"],"metadata":{"id":"_tDHQ6S96cyh","executionInfo":{"status":"ok","timestamp":1654602259577,"user_tz":-180,"elapsed":10,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def conceptNet(word):\n","  keyword = word.split(\" \")\n","  listWord = []\n","  for t in keyword:\n","    cleaned_weights, allData = [], []\n","    data1, data2, data3 = helperCN(t)\n","    prp = findSimilarity(data2, data1, [t], data3)\n","    first = prp.cleaning_entities()\n","    second, weights = prp.cleaning_entities2(first[1], first[0])\n","    cleaned_weights = prp.grounding(second, weights)\n","    listWord.append((t,cleaned_weights[:3]))\n","\n","  listWord = Remove(listWord)\n","  return listWord\n"],"metadata":{"id":"gObroD0S6h4d","executionInfo":{"status":"ok","timestamp":1654602259579,"user_tz":-180,"elapsed":11,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","\n","def simpleCN_list(terms, threshold):\n","    objects = [requests.get('http://api.conceptnet.io/related/c/en/'+t+'?filter=/c/en').json() for t in terms]\n","    llist = [[re.sub('/c/en/','',rel['@id']).split('_')for rel in obj['related'] if rel['weight'] > threshold] for obj in objects]\n","\n","    llist = [elem for sublist in llist for ind_list in sublist for elem in ind_list if elem not in stopwords.words('english') and elem not in terms ]\n","    return llist"],"metadata":{"id":"5hUAqru2WyAv","executionInfo":{"status":"ok","timestamp":1654602259580,"user_tz":-180,"elapsed":11,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#recommendationWN = wordNet('meat beef bike')##given a string, each word must be seperated by a space, it will then return the synonyms and the similarity score from wordnet for each word\n","#print(recommendationWN)\n","#recommendationCN = conceptNet('meat beef bike')##given a string, each word must be seperated by a space, it will then return the synonyms and the similarity score from conceptnet for each word, I have restricted it to 1.0 (top recommendations) otherwise there is to much noise\n","#print(recommendationCN)\n","#recommendationCN2 = simpleCN_list(['meat','beef','bike'],threshold=0.7)\n","#print(recommendationCN2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUQFtQLpxZ5W","executionInfo":{"status":"ok","timestamp":1654602265227,"user_tz":-180,"elapsed":5657,"user":{"displayName":"Kimon Spiliopoulos","userId":"12668773501214850994"}},"outputId":"50919acc-27b4-4784-9f19-dd0e18128b23"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[('meat', [('kernel', 0.3076923076923077)]), ('beef', [('gripe', 0.07692307692307693)]), ('bike', [('motorcycle', 1.0), ('bicycle', 0.7272727272727273)])]\n","[('meat', [('butcher', 4.47213595499958, 'RelatedTo'), ('meal', 2.82842712474619, 'Synonym'), ('stew', 2.0, 'RelatedTo')]), ('beef', [('food', 3.4641016151377544, 'RelatedTo'), ('cow', 3.118974190338869, 'RelatedTo'), ('meat', 2.0, 'RelatedTo')]), ('bike', [('motor', 2.0, 'RelatedTo'), ('motorcycle', 2.0, 'Synonym'), ('bicycle', 2.0, 'Synonym')])]\n","['meats', 'pigmeat', 'santa', 'gertrudis', 'cattle', 'beefs', 'beeves', 'beefing', 'ribeye', 'steak', 'hanger', 'steak', 'filet', 'mignon', 'prime', 'rib', 'veal', 'bikes', 'bicycle', 'pushbike', 'recumbent', 'bicycle', 'bicycled', 'biking', 'bicycle', 'motocross', 'bycicle', 'bicycles', 'motorcycle', 'motorbike', 'bicycling', 'bmx', 'dirtbike', 'minibike', 'cyclery', 'mountain', 'motorcycles', 'dirt', 'bicycle', 'shop', 'velocipede', 'mini', 'moto', 'cycling', 'reduce', 'carbon', 'emissions', 'cyclocross', 'bicyclist', 'fatbike']\n"]}]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37159f0c",
   "metadata": {},
   "source": [
    "# Statistics Explained  articles and matches with OECD's Glossary\n",
    "\n",
    "## Objective: to build a common vocabulary and construct \"profiles\" of the terms of this vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "## Run to install the language library, then comment-out\n",
    "#!{sys.executable} -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000\n",
    "print('Finished loading.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c82a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def file_name(pre,ext):\n",
    "    current_time = datetime.now() \n",
    "    return pre + '_'+ str(current_time.month)+ '_' + str(current_time.day) + \\\n",
    "                 '_' + str(current_time.hour)+ '_' + str(current_time.minute)  +'.'+ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093826ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "c = pyodbc.connect('DSN=Virtuoso All;DBA=ESTAT;UID=kimon;PWD=RkhvQYZ442e2JVXLHdtW')\n",
    "cursor = c.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import unicodedata as ud\n",
    "\n",
    "def clean(x, quotes=True):\n",
    "    if pd.isnull(x): return x  \n",
    "    x = x.strip()\n",
    "    \n",
    "    ## make letter-question mark-letter -> letter-quote-space-letter !!! but NOT in the lists of URLs!!!\n",
    "    if quotes:\n",
    "        x = re.sub(r'([A-Za-z])\\?([A-Za-z])','\\\\1\\' \\\\2',x) \n",
    "    \n",
    "    ## make letter-question mark-space lower case letter letter-quote-space letter\n",
    "    x = re.sub(r'([A-Za-z])\\? ([a-z])','\\\\1\\' \\\\2',x) \n",
    "\n",
    "    ## delete ,000 commas in numbers    \n",
    "    x = re.sub(r'\\b(\\d+),(\\d+)\\b','\\\\1\\\\2',x) ## CORRECTED\n",
    "    \n",
    "    ## delete  000 spaces in numbers\n",
    "    x = re.sub(r'\\b(\\d+) (\\d+)\\b','\\\\1\\\\2',x) ## CORRECTED\n",
    "    \n",
    "    ## remove more than one spaces\n",
    "    x = re.sub(r' +', ' ',x)\n",
    "    \n",
    "    ## remove start and end spaces\n",
    "    x = re.sub(r'^ +| +$', '',x,flags=re.MULTILINE) \n",
    "    \n",
    "    ## space-comma -> comma\n",
    "    x = re.sub(r' \\,',',',x)\n",
    "    \n",
    "    ## space-dot -> dot\n",
    "    x = re.sub(r' \\.','.',x)\n",
    "    \n",
    "    x = re.sub(r'â.{2}',\"'\",x) ### !!! NEW: single quotes are read as: âXX\n",
    "    \n",
    "    #x = x.encode('latin1').decode('utf-8') ## â\\x80\\x99\n",
    "    #x = ud.normalize('NFKD',x).encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af3a57",
   "metadata": {},
   "source": [
    "### Statistics explained articles\n",
    "\n",
    "* IDs, titles from dat_link_info, with resource_information_id=1, i.e. Eurostat (see ESTAT.V1.mod_resource_information) and matching IDs from dat_article.\n",
    "* Carry out data cleansing on titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLCommand = \"\"\"SELECT id, title \n",
    "                FROM ESTAT.V1.dat_link_info \n",
    "                WHERE resource_information_id=1 AND id IN (SELECT id FROM ESTAT.V1.dat_article) \"\"\"\n",
    "\n",
    "SE_df = pd.read_sql(SQLCommand,c)\n",
    "\n",
    "SE_df['title'] = SE_df['title'].apply(clean)\n",
    "SE_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1d102",
   "metadata": {},
   "source": [
    "### Add paragraphs titles and contents\n",
    "\n",
    "* From dat_article_paragraph with abstract=0 (i.e. \"no\").\n",
    "* Match article_id from dat_article_paragraph with id from dat_article.\n",
    "* Carry out data cleansing on titles and paragraph contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLCommand = \"\"\"SELECT article_id, title, content \n",
    "                FROM ESTAT.V1.dat_article_paragraph\n",
    "                WHERE abstract=0 AND article_id IN (SELECT id FROM ESTAT.V1.dat_article) \"\"\"\n",
    "\n",
    "add_content = pd.read_sql(SQLCommand,c)\n",
    "add_content['title'] = add_content['title'].apply(clean)\n",
    "add_content['content'] = add_content['content'].apply(clean)\n",
    "add_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb697d1",
   "metadata": {},
   "source": [
    "### Aggregate above paragraph titles and contents  from SE articles paragraphs by article id\n",
    "\n",
    "* Create a column _raw content_ which gathers all paragraph titles and contents in one text per article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_content_grouped = add_content.groupby(['article_id'])[['title','content']].aggregate(lambda x: list(x))\n",
    "add_content_grouped.reset_index(drop=False, inplace=True)\n",
    "for i in range(len(add_content_grouped)):\n",
    "    add_content_grouped.loc[i,'raw content'] = ''\n",
    "    for (a,b) in zip(add_content_grouped.loc[i,'title'],add_content_grouped.loc[i,'content']):\n",
    "        add_content_grouped.loc[i,'raw content'] += ' '+a + ' ' + b\n",
    "add_content_grouped = add_content_grouped[['article_id','raw content']]    \n",
    "\n",
    "add_content_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadf187",
   "metadata": {},
   "source": [
    "### Merge raw content of SE articles with main file\n",
    "\n",
    "* Also, add title to definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_df = pd.merge(SE_df,add_content_grouped,left_on='id',right_on='article_id',how='inner')\n",
    "SE_df.drop(['article_id'],axis=1,inplace=True)\n",
    "\n",
    "SE_df['raw content'] = SE_df['title'] +'. '+SE_df['raw content']\n",
    "\n",
    "SE_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfc3b3",
   "metadata": {},
   "source": [
    "### Lemmatize 'raw content'\n",
    "\n",
    "* NLTK seems to be better than Spacy in lemmatization. Convert to lower-case before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f65f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "SE_df['raw content'] = SE_df['raw content'].apply(lambda x: x.lower())\n",
    "SE_df['raw content']= SE_df['raw content'].apply(lemmatize_text)\n",
    "SE_df['raw content']= [' '.join(map(str, l)) for l in SE_df['raw content']]\n",
    "SE_df['raw content'] = SE_df['raw content'].apply(lambda x: x.upper())\n",
    "SE_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc804f",
   "metadata": {},
   "source": [
    "### OECD - Glossary of Statistical Terms\n",
    "https://stats.oecd.org/glossary/alpha.asp\n",
    "\n",
    "* Scrape terms and lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://stats.oecd.org/glossary/alpha.asp\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "text = soup.get_text()\n",
    "    \n",
    "rows = soup.find_all('tr')\n",
    "str_cells = str(rows)\n",
    "cleantext = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "#print(cleantext)\n",
    "\n",
    "list_rows = []\n",
    "for row in rows:\n",
    "    cells = row.find_all('a')\n",
    "    str_cells = str(cells)\n",
    "    clean = re.compile('<.*?>')\n",
    "    clean2 = (re.sub(clean, '',str_cells))\n",
    "    list_rows.append(clean2)\n",
    "#print(clean2)\n",
    "#type(clean2)\n",
    "\n",
    "df = pd.DataFrame(list_rows)\n",
    "df.head(10)\n",
    "df[0]=df[0].apply(lambda x: re.sub(r'\\[',' ',x))\n",
    "df[0]=df[0].apply(lambda x: re.sub(r'\\]',' ',x))\n",
    "df1 = df[0].str.split(',', expand=True)\n",
    "df_t = df1.T\n",
    "df_t=df_t[[22]]\n",
    "df_t = df_t.rename(columns={22: 'term'})\n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "df_t.replace(\" \", nan_value, inplace=True)\n",
    "\n",
    "df_t.dropna(subset = [\"term\"], inplace=True)\n",
    "df_t.replace(\" \", nan_value, inplace=True)\n",
    "df_t.insert(0, 'id', range(len(df_t)))\n",
    "df_t.reset_index(inplace=True)\n",
    "df_t.drop(columns=['index'],inplace=True)\n",
    "df_t.head()\n",
    "\n",
    "df_t['lemmatized_term']= df_t['term'].apply(lambda x: x.lower())\n",
    "df_t['lemmatized_term']= df_t['lemmatized_term'].apply(lemmatize_text)\n",
    "df_t['lemmatized_term']= [' '.join(map(str, l)) for l in df_t['lemmatized_term']]\n",
    "df_t['lemmatized_term']= df_t['lemmatized_term'].apply(lambda x: x.upper())\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f91f81",
   "metadata": {},
   "source": [
    "### Prepare Spacy's PhraseMatcher by building a custom vocabulary from OECD's Glossary (lemmatized_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = df_t['lemmatized_term'].values.tolist()\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ac7aa",
   "metadata": {},
   "source": [
    "### Apply PhraseMatcher\n",
    "\n",
    "* Collect results per SE article ('doc_id') in a dataframe res. Ignore matches with 2 words or less.\n",
    "* Depending on length of match: columns '3-Phrases', '4-Phrases', '5-and-above-Phrases'. These will contain dictionaries with the matched lemmatized terms and their counts, in descending order of counts.\n",
    "* Column 'Terms' has a dictionary with the corresponding **original terms** in OECD's Glossary and their counts in the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be507524",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index=range(len(SE_df)))\n",
    "res['3-Phrases']=[[] for i in range(len(SE_df))]\n",
    "res['4-Phrases']=[[] for i in range(len(SE_df))]\n",
    "res['5-and-above-Phrases']=[[] for i in range(len(SE_df))]\n",
    "res['Terms']=[dict() for i in range(len(SE_df))]\n",
    "docs=nlp.pipe(SE_df['raw content'])\n",
    "for (i,doc) in enumerate(docs):\n",
    "    print(i)\n",
    "    for sent in doc.sents:\n",
    "        matches = matcher(sent)\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            n_words = len(span.text.split(' '))\n",
    "            if n_words >= 3:\n",
    "                doc_id = SE_df.loc[i,'id']\n",
    "                idx = df_t.index[df_t['lemmatized_term'].str.contains(span.text,regex=False)].tolist()\n",
    "                print(i,SE_df.loc[i,'title'],len(sent.text),'>',n_words,span.text,idx)\n",
    "                res.loc[i,'doc_id']=doc_id\n",
    "                for elem in df_t.loc[idx,'term'].values.tolist():\n",
    "                    if elem in res.loc[i,'Terms'].keys():\n",
    "                        res.loc[i,'Terms'][elem] +=1\n",
    "                    else:\n",
    "                        res.loc[i,'Terms'][elem] =1\n",
    "                #res.loc[i,'Terms'].append(concepts_df.loc[idx,'term'].values.tolist())\n",
    "                if n_words == 3:\n",
    "                    res.loc[i,'3-Phrases'].append(span.text)\n",
    "                elif n_words == 4:\n",
    "                    res.loc[i,'4-Phrases'].append(span.text)\n",
    "                else:\n",
    "                    res.loc[i,'5-and-above-Phrases'].append(span.text)\n",
    "                    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['3-Phrases']= res['3-Phrases'].apply(lambda x: dict(Counter(x).most_common()))\n",
    "res['4-Phrases']= res['4-Phrases'].apply(lambda x: dict(Counter(x).most_common()))\n",
    "res['5-and-above-Phrases']= res['5-and-above-Phrases'].apply(lambda x: dict(Counter(x).most_common()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6766fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.merge(SE_df[['id','title']],res,left_on='id',right_on='doc_id',how='left')\n",
    "res.drop(columns=['doc_id'],inplace=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = file_name('Phrase_Matcher_SE_OECD','xlsx')\n",
    "res.to_excel(outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
